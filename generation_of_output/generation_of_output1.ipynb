{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489a99bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results root: ./logit_lens_results\n",
      "\n",
      "=== Loading model: /root/autodl-tmp/llama ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f30b210068483fbfc34bda3ea313fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer blocks: 32; hidden_states count: 33\n",
      "\n",
      "--- Running Experiment with Seed: 42 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 42 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 43 ---\n",
      "Seed 43 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 44 ---\n",
      "Seed 44 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 45 ---\n",
      "Seed 45 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 46 ---\n",
      "Seed 46 done. 'Never Top-1' samples: 0\n",
      "\n",
      "Visualizing aggregated results (last layers)...\n",
      "Saved figure to → ./logit_lens_results/llama/logit_lens_histogram_last10L.pdf\n",
      "Finished model: llama. Results in: ./logit_lens_results/llama\n",
      "\n",
      "=== Loading model: /root/autodl-tmp/Mistral ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29aa86a3ea44200bca6df812f220b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer blocks: 32; hidden_states count: 33\n",
      "\n",
      "--- Running Experiment with Seed: 42 ---\n",
      "Seed 42 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 43 ---\n",
      "Seed 43 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 44 ---\n",
      "Seed 44 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 45 ---\n",
      "Seed 45 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 46 ---\n",
      "Seed 46 done. 'Never Top-1' samples: 0\n",
      "\n",
      "Visualizing aggregated results (last layers)...\n",
      "Saved figure to → ./logit_lens_results/Mistral/logit_lens_histogram_last10L.pdf\n",
      "Finished model: Mistral. Results in: ./logit_lens_results/Mistral\n",
      "\n",
      "=== Loading model: /root/autodl-tmp/AceMath ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e8d1876e8d42908e29eebb9f984e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer blocks: 28; hidden_states count: 29\n",
      "\n",
      "--- Running Experiment with Seed: 42 ---\n",
      "Seed 42 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 43 ---\n",
      "Seed 43 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 44 ---\n",
      "Seed 44 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 45 ---\n",
      "Seed 45 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 46 ---\n",
      "Seed 46 done. 'Never Top-1' samples: 0\n",
      "\n",
      "Visualizing aggregated results (last layers)...\n",
      "Saved figure to → ./logit_lens_results/AceMath/logit_lens_histogram_last10L.pdf\n",
      "Finished model: AceMath. Results in: ./logit_lens_results/AceMath\n",
      "\n",
      "=== Loading model: /root/autodl-tmp/Qwen2.5-Math-7B ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed35fc10a3cd4cfca0f9a949c8856fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer blocks: 28; hidden_states count: 29\n",
      "\n",
      "--- Running Experiment with Seed: 42 ---\n",
      "Seed 42 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 43 ---\n",
      "Seed 43 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 44 ---\n",
      "Seed 44 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 45 ---\n",
      "Seed 45 done. 'Never Top-1' samples: 0\n",
      "\n",
      "--- Running Experiment with Seed: 46 ---\n",
      "Seed 46 done. 'Never Top-1' samples: 0\n",
      "\n",
      "Visualizing aggregated results (last layers)...\n",
      "Saved figure to → ./logit_lens_results/Qwen2.5-Math-7B/logit_lens_histogram_last10L.pdf\n",
      "Finished model: Qwen2.5-Math-7B. Results in: ./logit_lens_results/Qwen2.5-Math-7B\n",
      "\n",
      "=== Loading model: /root/autodl-tmp/Qwen2.5-7B-Instruct ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d82ae1455a042679f43acd292802fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer blocks: 28; hidden_states count: 29\n",
      "\n",
      "--- Running Experiment with Seed: 42 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 42 done. 'Never Top-1' samples: 237\n",
      "\n",
      "--- Running Experiment with Seed: 43 ---\n",
      "Seed 43 done. 'Never Top-1' samples: 216\n",
      "\n",
      "--- Running Experiment with Seed: 44 ---\n",
      "Seed 44 done. 'Never Top-1' samples: 218\n",
      "\n",
      "--- Running Experiment with Seed: 45 ---\n",
      "Seed 45 done. 'Never Top-1' samples: 252\n",
      "\n",
      "--- Running Experiment with Seed: 46 ---\n",
      "Seed 46 done. 'Never Top-1' samples: 221\n",
      "\n",
      "Visualizing aggregated results (last layers)...\n",
      "Saved figure to → ./logit_lens_results/Qwen2.5-7B-Instruct/logit_lens_histogram_last10L.pdf\n",
      "Finished model: Qwen2.5-7B-Instruct. Results in: ./logit_lens_results/Qwen2.5-7B-Instruct\n",
      "\n",
      "All models finished.\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib  # for colormap access\n",
    "\n",
    "# =========================\n",
    "# 全局超参（可按需调整）\n",
    "# =========================\n",
    "MODEL_PATHS = [\n",
    "        \"/root/autodl-tmp/llama\",\n",
    "        \"/root/autodl-tmp/Mistral\",\n",
    "        \"/root/autodl-tmp/AceMath\",\n",
    "        \"/root/autodl-tmp/Qwen2.5-Math-7B\",\n",
    "        \"/root/autodl-tmp/Qwen2.5-7B-Instruct\"\n",
    "    ]\n",
    "OUTPUT_ROOT_DIR = \"./logit_lens_results\"\n",
    "\n",
    "NUM_DATASET_SAMPLES = 1000       # 每次实验的算术题数量\n",
    "NUM_REPETITIONS = 5              # 重复实验次数\n",
    "BASE_SEED = 42                   # 基础随机种子（每次重复+1）\n",
    "\n",
    "# 生成相关\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_NEW_TOKENS = 1\n",
    "DO_SAMPLE = False\n",
    "\n",
    "# 可视化相关\n",
    "DISPLAY_LAST_N_LAYERS = 10\n",
    "FIGSIZE = (12, 7)\n",
    "\n",
    "# =========================\n",
    "# 工具函数\n",
    "# =========================\n",
    "def set_seeds(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def sanitize_name(s: str) -> str:\n",
    "    # 把 HuggingFace id 或路径名转成安全目录名\n",
    "    s = s.strip().strip(\"/\\\\\")\n",
    "    s = os.path.basename(s) or s\n",
    "    s = s.replace(\"/\", \"__\").replace(\"\\\\\", \"__\")\n",
    "    return re.sub(r\"[^A-Za-z0-9._\\-+=@]\", \"_\", s)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def generate_addition_problem():\n",
    "    \"\"\"\n",
    "    生成两个三位数的加法题，且和仍为三位数。\n",
    "    形如：'Calculate: X + Y = '\n",
    "    \"\"\"\n",
    "    x = random.randint(100, 899)\n",
    "    y_lower_bound = 100\n",
    "    y_upper_bound = 999 - x\n",
    "    if y_upper_bound < y_lower_bound:\n",
    "        return None\n",
    "    y = random.randint(y_lower_bound, y_upper_bound)\n",
    "    return f\"Calculate: {x} + {y} = \"\n",
    "\n",
    "# =========================\n",
    "# Matplotlib 全局风格\n",
    "# =========================\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'font.family': 'serif',\n",
    "    'font.sans-serif': ['Arial'],\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 16,\n",
    "    'lines.linewidth': 1.8,\n",
    "    'lines.markersize': 6,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.5,\n",
    "    'grid.linestyle': ':',\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.format': 'pdf',\n",
    "    'savefig.bbox': 'tight',\n",
    "})\n",
    "try:\n",
    "    colors_cmap = matplotlib.colormaps.get_cmap('tab10')\n",
    "except AttributeError:\n",
    "    colors_cmap = plt.cm.get_cmap('tab10')\n",
    "bar_plot_color = colors_cmap(0)\n",
    "\n",
    "# =========================\n",
    "# 单次实验（对一个模型的一次重复）\n",
    "# =========================\n",
    "def run_single_experiment(current_seed, model_obj, tokenizer_obj, device_obj, num_samples, num_total_layers):\n",
    "    set_seeds(current_seed)\n",
    "    print(f\"\\n--- Running Experiment with Seed: {current_seed} ---\")\n",
    "\n",
    "    # 1) 生成题目\n",
    "    prompts, attempts = [], 0\n",
    "    max_attempts = num_samples * 2\n",
    "    while len(prompts) < num_samples and attempts < max_attempts:\n",
    "        problem = generate_addition_problem()\n",
    "        if problem:\n",
    "            prompts.append(problem)\n",
    "        attempts += 1\n",
    "    if not prompts:\n",
    "        print(\"Error: No prompts were generated for this run. Skipping.\")\n",
    "        return [-1] * num_samples, [], [], num_samples\n",
    "    if len(prompts) < num_samples:\n",
    "        print(f\"Warning: Only generated {len(prompts)} prompts out of {num_samples} requested for seed {current_seed}.\")\n",
    "\n",
    "    # 2) 预测下一个 token（模型自己的下一步）\n",
    "    generation_inputs = tokenizer_obj(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_INPUT_LENGTH).to(device_obj)\n",
    "    with torch.no_grad():\n",
    "        generated_outputs = model_obj.generate(\n",
    "            input_ids=generation_inputs[\"input_ids\"],\n",
    "            attention_mask=generation_inputs[\"attention_mask\"],\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            pad_token_id=tokenizer_obj.pad_token_id\n",
    "        )\n",
    "\n",
    "    original_input_lengths = generation_inputs[\"input_ids\"].shape[1]\n",
    "    target_token_ids = generated_outputs[:, original_input_lengths:].squeeze(-1)\n",
    "    target_token_strs = [tokenizer_obj.decode(token_id, skip_special_tokens=True) for token_id in target_token_ids]\n",
    "\n",
    "    # 3) 准备 Logit Lens 分析输入\n",
    "    input_ids_for_analysis = generation_inputs[\"input_ids\"]\n",
    "    attention_mask_for_analysis = generation_inputs[\"attention_mask\"]\n",
    "\n",
    "    # 4) 逐层计算 hidden states，找出目标 token 首次成为 Top-1 的层\n",
    "    with torch.no_grad():\n",
    "        outputs = model_obj(\n",
    "            input_ids=input_ids_for_analysis,\n",
    "            attention_mask=attention_mask_for_analysis,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "    hidden_states_all_layers = outputs.hidden_states  # len = num_total_layers (含 embedding 0)\n",
    "    lm_head = model_obj.lm_head\n",
    "    sequence_lengths = torch.sum(attention_mask_for_analysis, dim=1) - 1\n",
    "    batch_size = input_ids_for_analysis.shape[0]\n",
    "    first_top1_layer_per_sample = [-1] * batch_size\n",
    "\n",
    "    for layer_idx, layer_hidden_states in enumerate(hidden_states_all_layers):\n",
    "        if layer_idx >= num_total_layers:\n",
    "            break\n",
    "        batch_indices = torch.arange(batch_size, device=device_obj)\n",
    "        last_token_hidden_states = layer_hidden_states[batch_indices, sequence_lengths, :]\n",
    "        logits_at_layer = lm_head(last_token_hidden_states)\n",
    "        top1_pred_ids = torch.argmax(logits_at_layer, dim=1)\n",
    "\n",
    "        for sample_idx in range(batch_size):\n",
    "            if (first_top1_layer_per_sample[sample_idx] == -1 and\n",
    "                top1_pred_ids[sample_idx].item() == target_token_ids[sample_idx].item()):\n",
    "                first_top1_layer_per_sample[sample_idx] = layer_idx\n",
    "\n",
    "    num_never_top1_this_run = first_top1_layer_per_sample.count(-1)\n",
    "    print(f\"Seed {current_seed} done. 'Never Top-1' samples: {num_never_top1_this_run}\")\n",
    "    return first_top1_layer_per_sample, target_token_strs, prompts, num_never_top1_this_run\n",
    "\n",
    "# =========================\n",
    "# 单模型多次重复 + 汇总 + 输出\n",
    "# =========================\n",
    "def run_for_one_model(model_path: str):\n",
    "    model_id = sanitize_name(model_path)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT_DIR, model_id)\n",
    "    ensure_dir(out_dir)\n",
    "    print(f\"\\n=== Loading model: {model_path} ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    num_layers = model.config.num_hidden_layers + 1\n",
    "    print(f\"Transformer blocks: {model.config.num_hidden_layers}; hidden_states count: {num_layers}\")\n",
    "\n",
    "    # ---- 重复实验 ----\n",
    "    all_runs_first_top1_layers_data = []\n",
    "    all_runs_num_never_top1_counts = []\n",
    "\n",
    "    for i in range(NUM_REPETITIONS):\n",
    "        current_run_seed = BASE_SEED + i\n",
    "        first_top1_data, _, _, num_never_top1 = run_single_experiment(\n",
    "            current_run_seed, model, tokenizer, device, NUM_DATASET_SAMPLES, num_layers\n",
    "        )\n",
    "        all_runs_first_top1_layers_data.append(first_top1_data)\n",
    "        all_runs_num_never_top1_counts.append(num_never_top1)\n",
    "\n",
    "        # 保存每次 run 的原始结果（CSV）\n",
    "        csv_path = os.path.join(out_dir, f\"first_top1_layers_run{i+1}.csv\")\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"sample_idx,first_top1_layer\\n\")\n",
    "            for idx, layer in enumerate(first_top1_data):\n",
    "                f.write(f\"{idx},{layer}\\n\")\n",
    "\n",
    "    # ---- 汇总统计 ----\n",
    "    aggregated_layer_counts_matrix = np.zeros((NUM_REPETITIONS, num_layers), dtype=float)\n",
    "    for run_idx, single_run_results in enumerate(all_runs_first_top1_layers_data):\n",
    "        if len(single_run_results) != NUM_DATASET_SAMPLES:\n",
    "            print(f\"Warning: Run {run_idx} returned {len(single_run_results)} results; expected {NUM_DATASET_SAMPLES}.\")\n",
    "        counts_for_this_run = Counter(l for l in single_run_results if l != -1)\n",
    "        for layer_idx_val, count_val in counts_for_this_run.items():\n",
    "            if 0 <= layer_idx_val < num_layers:\n",
    "                aggregated_layer_counts_matrix[run_idx, layer_idx_val] = count_val\n",
    "\n",
    "    mean_counts_per_layer = np.mean(aggregated_layer_counts_matrix, axis=0)\n",
    "    std_dev_counts_per_layer = np.std(aggregated_layer_counts_matrix, axis=0)\n",
    "    std_err_counts_per_layer = std_dev_counts_per_layer / np.sqrt(NUM_REPETITIONS)\n",
    "\n",
    "    mean_num_never_top1_overall = float(np.mean(all_runs_num_never_top1_counts))\n",
    "    std_dev_num_never_top1_overall = float(np.std(all_runs_num_never_top1_counts))\n",
    "    std_err_num_never_top1_overall = std_dev_num_never_top1_overall / np.sqrt(NUM_REPETITIONS)\n",
    "\n",
    "    # 保存汇总 CSV\n",
    "    agg_csv = os.path.join(out_dir, \"aggregated_counts_per_layer.csv\")\n",
    "    with open(agg_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"layer_idx,mean_count,std_err\\n\")\n",
    "        for li in range(num_layers):\n",
    "            f.write(f\"{li},{mean_counts_per_layer[li]:.6f},{std_err_counts_per_layer[li]:.6f}\\n\")\n",
    "\n",
    "    # 保存摘要 JSON（含超参、层数、统计）\n",
    "    summary_json = os.path.join(out_dir, \"summary.json\")\n",
    "    summary = {\n",
    "        \"model_id\": model_id,\n",
    "        \"model_path\": model_path,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"params\": {\n",
    "            \"NUM_DATASET_SAMPLES\": NUM_DATASET_SAMPLES,\n",
    "            \"NUM_REPETITIONS\": NUM_REPETITIONS,\n",
    "            \"BASE_SEED\": BASE_SEED,\n",
    "            \"MAX_INPUT_LENGTH\": MAX_INPUT_LENGTH,\n",
    "            \"MAX_NEW_TOKENS\": MAX_NEW_TOKENS,\n",
    "            \"DO_SAMPLE\": DO_SAMPLE,\n",
    "        },\n",
    "        \"num_layers\": num_layers,\n",
    "        \"display_last_n_layers\": min(DISPLAY_LAST_N_LAYERS, num_layers),\n",
    "        \"mean_counts_per_layer\": mean_counts_per_layer.tolist(),\n",
    "        \"std_err_counts_per_layer\": std_err_counts_per_layer.tolist(),\n",
    "        \"never_top1_counts_per_run\": all_runs_num_never_top1_counts,\n",
    "        \"mean_never_top1\": mean_num_never_top1_overall,\n",
    "        \"stddev_never_top1\": std_dev_num_never_top1_overall,\n",
    "        \"stderr_never_top1\": std_err_num_never_top1_overall,\n",
    "    }\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ---- 画图并保存到该模型目录 ----\n",
    "    print(\"\\nVisualizing aggregated results (last layers)...\")\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "    start_layer_for_display = max(0, num_layers - DISPLAY_LAST_N_LAYERS)\n",
    "    layers_to_plot = list(range(start_layer_for_display, num_layers))\n",
    "    actual_displayed_n_layers = len(layers_to_plot)\n",
    "\n",
    "    mean_counts_for_displayed_layers = [mean_counts_per_layer[idx] for idx in layers_to_plot]\n",
    "    error_for_displayed_layers = [std_err_counts_per_layer[idx] for idx in layers_to_plot]\n",
    "\n",
    "    if not layers_to_plot:\n",
    "        ax.text(0.5, 0.5,\n",
    "                f\"Model has only {num_layers} layer(s).\\nCannot display the last {DISPLAY_LAST_N_LAYERS} layers.\",\n",
    "                ha='center', va='center', transform=ax.transAxes,\n",
    "                bbox=dict(boxstyle='round,pad=0.5', fc='lightyellow', alpha=0.8))\n",
    "    else:\n",
    "        bars = ax.bar(\n",
    "            layers_to_plot,\n",
    "            mean_counts_for_displayed_layers,\n",
    "            yerr=error_for_displayed_layers,\n",
    "            capsize=4,\n",
    "            color=bar_plot_color,\n",
    "            zorder=2,\n",
    "            width=0.8,\n",
    "            edgecolor='black',\n",
    "            linewidth=0.7\n",
    "        )\n",
    "        ax.set_xticks(layers_to_plot)\n",
    "        ax.set_xticklabels([str(l) for l in layers_to_plot])\n",
    "\n",
    "        max_mean_count_in_display = max(mean_counts_for_displayed_layers, default=1) if mean_counts_for_displayed_layers else 1\n",
    "        for bar_obj, mean_val in zip(bars, mean_counts_for_displayed_layers):\n",
    "            height = bar_obj.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(\n",
    "                    bar_obj.get_x() + bar_obj.get_width() / 2.0,\n",
    "                    height + 0.02 * max_mean_count_in_display,\n",
    "                    f'{mean_val:.1f}',\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=plt.rcParams['xtick.labelsize'] - 1\n",
    "                )\n",
    "\n",
    "        num_samples_reached_top1_overall_avg = float(np.sum(mean_counts_per_layer))\n",
    "        if NUM_DATASET_SAMPLES > 0 and sum(mean_counts_for_displayed_layers) < 0.1:\n",
    "            extra_info = \"\"\n",
    "            if num_samples_reached_top1_overall_avg > 0.1:\n",
    "                extra_info = (f\"\\n(Note: Avg. {num_samples_reached_top1_overall_avg:.1f} sample(s) reached Top-1 \"\n",
    "                              f\"in earlier layers not shown.)\")\n",
    "            elif num_samples_reached_top1_overall_avg < 0.1:\n",
    "                extra_info = (f\"\\n(Avg. {num_samples_reached_top1_overall_avg:.1f} samples reached Top-1 in any layer.)\")\n",
    "            ax.text(\n",
    "                0.5, 0.5,\n",
    "                (f\"Avg. count of samples first becoming Top-1 is near zero\\n\"\n",
    "                 f\"in the displayed layers ({start_layer_for_display}-{num_layers-1}).{extra_info}\"),\n",
    "                ha='center', va='center', transform=ax.transAxes,\n",
    "                fontsize=plt.rcParams['legend.fontsize'],\n",
    "                bbox=dict(boxstyle='round,pad=0.3', fc='lightyellow', alpha=0.8)\n",
    "            )\n",
    "\n",
    "    title_line1 = (f\"{model_id} | First layer where model's predicted token becomes Top-1 \"\n",
    "                   f\"(Shown: {start_layer_for_display}-{num_layers-1})\")\n",
    "    title_line2 = (f\"3-digit additions | Repetitions: {NUM_REPETITIONS}\")\n",
    "    title_line3 = (f\"Never Top-1: {mean_num_never_top1_overall:.1f} ± \"\n",
    "                   f\"{std_err_num_never_top1_overall:.1f} (SE)\")\n",
    "\n",
    "    ax.set_title(f\"{title_line1}\\n{title_line2}\\n{title_line3}\", wrap=True, fontsize=16)\n",
    "    ax.set_xlabel(f\"Layer Index (Range shown: {start_layer_for_display} to {num_layers-1})\")\n",
    "    ax.set_ylabel(f\"Avg. # Samples (First Top-1 in this layer, ±SE over {NUM_REPETITIONS} runs)\")\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(direction='in', top=False, right=False)\n",
    "    ax.yaxis.grid(True, linestyle=plt.rcParams['grid.linestyle'], linewidth=0.6, alpha=plt.rcParams['grid.alpha'])\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    if mean_num_never_top1_overall >= 0:\n",
    "        never_top1_text = (f\"Avg. 'Never Top-1': {mean_num_never_top1_overall:.1f} \"\n",
    "                           f\"± {std_err_num_never_top1_overall:.1f} (SE)\\n\"\n",
    "                           f\"over {NUM_REPETITIONS} runs.\")\n",
    "        ax.text(\n",
    "            0.98, 0.05, never_top1_text,\n",
    "            ha='right', va='bottom', transform=ax.transAxes,\n",
    "            fontsize=plt.rcParams['legend.fontsize'] - 1, bbox=dict(boxstyle='round,pad=0.3', fc='lightcoral', alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout(pad=1.0)\n",
    "    fig_path = os.path.join(out_dir, f\"logit_lens_histogram_last{len(layers_to_plot)}L.pdf\")\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved figure to → {fig_path}\")\n",
    "\n",
    "    # 释放显存/内存，便于下一个模型\n",
    "    del model, tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Finished model: {model_id}. Results in: {out_dir}\")\n",
    "\n",
    "# =========================\n",
    "# 主入口：批量跑多个模型\n",
    "# =========================\n",
    "def main():\n",
    "    ensure_dir(OUTPUT_ROOT_DIR)\n",
    "    print(f\"Results root: {OUTPUT_ROOT_DIR}\")\n",
    "    for mp in MODEL_PATHS:\n",
    "        run_for_one_model(mp)\n",
    "    print(\"\\nAll models finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
