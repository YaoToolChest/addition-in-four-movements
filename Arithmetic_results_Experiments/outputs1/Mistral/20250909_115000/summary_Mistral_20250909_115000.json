{
  "model": "/root/autodl-tmp/Mistral",
  "model_short": "Mistral",
  "timestamp": "20250909_115000",
  "device": "cuda:0",
  "hyper_params": {
    "MODEL_NAMES": [
      "/root/autodl-tmp/Mistral"
    ],
    "N_REPETITIONS": 5,
    "BASE_SEED": 42,
    "PROBE_TYPE": "linear",
    "PROBE_MLP_HIDDEN_DIM": 4096,
    "N_PROBE_EPOCHS": 5,
    "PROBE_LR": 0.001,
    "PROBE_BATCH_SIZE": 64,
    "TEST_SPLIT_RATIO": 0.2,
    "GENERATION_BATCH_SIZE": 500,
    "GEN_MAX_NEW_TOKENS": 3,
    "GEN_DO_SAMPLE": false,
    "TOK_MAX_LENGTH": 60,
    "TOK_PADDING_SIDE": "left",
    "SUM_RANGES_CONFIG": [
      {
        "label": "500-509",
        "start": 500,
        "count": 10
      },
      {
        "label": "600-609",
        "start": 600,
        "count": 10
      },
      {
        "label": "700-709",
        "start": 700,
        "count": 10
      },
      {
        "label": "800-809",
        "start": 800,
        "count": 10
      },
      {
        "label": "900-909",
        "start": 900,
        "count": 10
      }
    ],
    "N_SAMPLES_PER_SUM": 400,
    "MIN_SAMPLES_PER_CLASS_FOR_PROBING": 5,
    "MIN_ADDEND_VAL": 100,
    "A_BOUND_LOW_FRAC": 0.25,
    "A_BOUND_HIGH_FRAC": 0.75,
    "MAX_ATTEMPTS_MULTIPLIER": 20,
    "ACTIVATIONS_BATCH_SIZE": 256,
    "LOG_TO_CONSOLE": false,
    "TQDM_DISABLED": true,
    "SUPPRESS_WARNINGS": true
  },
  "probe_type": "linear",
  "probe_epochs": 5,
  "probe_lr": 0.001,
  "probe_batch_size": 64,
  "generation_batch_size": 500,
  "sum_ranges_config": [
    {
      "label": "500-509",
      "start": 500,
      "count": 10
    },
    {
      "label": "600-609",
      "start": 600,
      "count": 10
    },
    {
      "label": "700-709",
      "start": 700,
      "count": 10
    },
    {
      "label": "800-809",
      "start": 800,
      "count": 10
    },
    {
      "label": "900-909",
      "start": 900,
      "count": 10
    }
  ],
  "n_samples_per_sum": 400,
  "test_split_ratio": 0.2,
  "layers_to_probe": [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32
  ],
  "logical_num_layers": 32,
  "hidden_states_tuple_len": 33,
  "results": {
    "500-509": {
      "target_sums": [
        500,
        501,
        502,
        503,
        504,
        505,
        506,
        507,
        508,
        509
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.67525,
        0.665,
        0.6775,
        0.6815,
        0.6765
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2701,
        2660,
        2710,
        2726,
        2706
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.14048059149722736,
          0.14285714285714285,
          0.14206642066420663,
          0.14102564102564102,
          0.14391143911439114
        ],
        "2": [
          0.14417744916820702,
          0.14285714285714285,
          0.14206642066420663,
          0.14102564102564102,
          0.14391143911439114
        ],
        "3": [
          0.14048059149722736,
          0.14097744360902256,
          0.14022140221402213,
          0.14102564102564102,
          0.14391143911439114
        ],
        "4": [
          0.1423290203327172,
          0.14285714285714285,
          0.14206642066420663,
          0.13553113553113552,
          0.14391143911439114
        ],
        "5": [
          0.1423290203327172,
          0.14285714285714285,
          0.14022140221402213,
          0.14102564102564102,
          0.14206642066420663
        ],
        "6": [
          0.1423290203327172,
          0.14285714285714285,
          0.14206642066420663,
          0.15018315018315018,
          0.13837638376383765
        ],
        "7": [
          0.13863216266173753,
          0.14285714285714285,
          0.14206642066420663,
          0.1391941391941392,
          0.12915129151291513
        ],
        "8": [
          0.14417744916820702,
          0.14285714285714285,
          0.1752767527675277,
          0.14835164835164835,
          0.14760147601476015
        ],
        "9": [
          0.14048059149722736,
          0.16541353383458646,
          0.1752767527675277,
          0.14835164835164835,
          0.15682656826568267
        ],
        "10": [
          0.1478743068391867,
          0.14285714285714285,
          0.14022140221402213,
          0.14652014652014653,
          0.14206642066420663
        ],
        "11": [
          0.21256931608133087,
          0.14285714285714285,
          0.14575645756457564,
          0.152014652014652,
          0.17158671586715868
        ],
        "12": [
          0.16635859519408502,
          0.14285714285714285,
          0.15498154981549817,
          0.21978021978021978,
          0.15682656826568267
        ],
        "13": [
          0.17929759704251386,
          0.14661654135338345,
          0.1752767527675277,
          0.1978021978021978,
          0.16605166051660517
        ],
        "14": [
          0.18299445471349354,
          0.14849624060150377,
          0.16420664206642066,
          0.2087912087912088,
          0.1937269372693727
        ],
        "15": [
          0.21256931608133087,
          0.20864661654135339,
          0.1863468634686347,
          0.20695970695970695,
          0.2047970479704797
        ],
        "16": [
          0.2033271719038817,
          0.18984962406015038,
          0.1752767527675277,
          0.18864468864468864,
          0.22509225092250923
        ],
        "17": [
          0.19593345656192238,
          0.2199248120300752,
          0.2158671586715867,
          0.2032967032967033,
          0.17712177121771217
        ],
        "18": [
          0.22735674676524953,
          0.20112781954887218,
          0.1881918819188192,
          0.21062271062271062,
          0.2158671586715867
        ],
        "19": [
          0.2199630314232902,
          0.17857142857142858,
          0.17712177121771217,
          0.19597069597069597,
          0.21033210332103322
        ],
        "20": [
          0.21811460258780038,
          0.20300751879699247,
          0.1974169741697417,
          0.1978021978021978,
          0.2140221402214022
        ],
        "21": [
          0.23475046210720887,
          0.20488721804511278,
          0.1992619926199262,
          0.2087912087912088,
          0.16789667896678967
        ],
        "22": [
          0.2513863216266174,
          0.19172932330827067,
          0.2047970479704797,
          0.16666666666666666,
          0.21955719557195572
        ],
        "23": [
          0.19963031423290203,
          0.21052631578947367,
          0.23247232472324722,
          0.22527472527472528,
          0.21033210332103322
        ],
        "24": [
          0.24029574861367836,
          0.22180451127819548,
          0.2140221402214022,
          0.2271062271062271,
          0.22878228782287824
        ],
        "25": [
          0.24399260628465805,
          0.16541353383458646,
          0.2564575645756458,
          0.22344322344322345,
          0.22878228782287824
        ],
        "26": [
          0.24029574861367836,
          0.19924812030075187,
          0.2140221402214022,
          0.2032967032967033,
          0.23800738007380073
        ],
        "27": [
          0.24953789279112754,
          0.212406015037594,
          0.23062730627306274,
          0.23076923076923078,
          0.24907749077490776
        ],
        "28": [
          0.2587800369685767,
          0.24812030075187969,
          0.25092250922509224,
          0.2619047619047619,
          0.23616236162361623
        ],
        "29": [
          0.18853974121996303,
          0.22932330827067668,
          0.25092250922509224,
          0.22527472527472528,
          0.33210332103321033
        ],
        "30": [
          0.30129390018484287,
          0.2819548872180451,
          0.2730627306273063,
          0.28205128205128205,
          0.27490774907749077
        ],
        "31": [
          0.31053604436229204,
          0.25375939849624063,
          0.24723247232472326,
          0.2216117216117216,
          0.2859778597785978
        ],
        "32": [
          0.36968576709796674,
          0.32142857142857145,
          0.27490774907749077,
          0.19230769230769232,
          0.2915129151291513
        ]
      },
      "best_layer_indices_reps": [
        32,
        32,
        32,
        30,
        29
      ],
      "best_layer_accuracies_reps": [
        0.36968576709796674,
        0.32142857142857145,
        0.27490774907749077,
        0.28205128205128205,
        0.33210332103321033
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.64      0.29      0.40        24\n           1       0.15      0.08      0.11        25\n           2       0.30      0.28      0.29        40\n           3       0.53      0.19      0.28        43\n           4       0.25      0.40      0.31        57\n           5       0.00      0.00      0.00        60\n           6       0.28      0.78      0.42        69\n           7       0.22      0.14      0.17        70\n           8       0.66      0.32      0.43        77\n           9       0.60      0.79      0.68        76\n\n    accuracy                           0.37       541\n   macro avg       0.36      0.33      0.31       541\nweighted avg       0.37      0.37      0.33       541\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.15      0.26        20\n           1       0.64      0.41      0.50        22\n           2       0.20      0.62      0.30        34\n           3       0.30      0.18      0.23        44\n           4       0.32      0.48      0.38        62\n           5       0.34      0.27      0.30        60\n           6       0.00      0.00      0.00        69\n           7       0.67      0.03      0.06        69\n           8       0.57      0.11      0.18        76\n           9       0.33      0.97      0.49        76\n\n    accuracy                           0.32       532\n   macro avg       0.44      0.32      0.27       532\nweighted avg       0.39      0.32      0.25       532\n",
        "              precision    recall  f1-score   support\n\n           0       0.58      0.61      0.60        23\n           1       0.24      0.39      0.30        28\n           2       0.00      0.00      0.00        36\n           3       0.44      0.16      0.24        43\n           4       0.27      0.36      0.31        58\n           5       0.00      0.00      0.00        62\n           6       0.18      0.37      0.25        70\n           7       0.00      0.00      0.00        69\n           8       0.29      0.87      0.44        77\n           9       1.00      0.04      0.08        76\n\n    accuracy                           0.27       542\n   macro avg       0.30      0.28      0.22       542\nweighted avg       0.31      0.27      0.20       542\n",
        "              precision    recall  f1-score   support\n\n           0       0.24      0.63      0.35        19\n           1       0.00      0.00      0.00        29\n           2       0.20      0.09      0.12        34\n           3       0.53      0.17      0.26        46\n           4       0.21      0.15      0.17        61\n           5       0.00      0.00      0.00        64\n           6       0.21      0.79      0.33        70\n           7       0.36      0.14      0.20        72\n           8       0.00      0.00      0.00        77\n           9       0.43      0.77      0.55        74\n\n    accuracy                           0.28       546\n   macro avg       0.22      0.27      0.20       546\nweighted avg       0.22      0.28      0.21       546\n",
        "              precision    recall  f1-score   support\n\n           0       0.38      0.14      0.20        22\n           1       0.29      0.15      0.20        26\n           2       0.17      0.03      0.05        38\n           3       0.29      0.52      0.37        44\n           4       0.22      0.42      0.29        59\n           5       0.28      0.20      0.23        61\n           6       0.28      0.26      0.27        68\n           7       0.47      0.10      0.16        71\n           8       0.44      0.46      0.45        78\n           9       0.45      0.68      0.54        75\n\n    accuracy                           0.33       542\n   macro avg       0.32      0.30      0.28       542\nweighted avg       0.34      0.33      0.30       542\n"
      ]
    },
    "600-609": {
      "target_sums": [
        600,
        601,
        602,
        603,
        604,
        605,
        606,
        607,
        608,
        609
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.82125,
        0.821,
        0.8115,
        0.8175,
        0.8145
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        3285,
        3284,
        3246,
        3270,
        3258
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.1202435312024353,
          0.121765601217656,
          0.12307692307692308,
          0.12232415902140673,
          0.12269938650306748
        ],
        "2": [
          0.1156773211567732,
          0.1171993911719939,
          0.12307692307692308,
          0.11926605504587157,
          0.12116564417177914
        ],
        "3": [
          0.121765601217656,
          0.121765601217656,
          0.12307692307692308,
          0.10856269113149847,
          0.12269938650306748
        ],
        "4": [
          0.1202435312024353,
          0.1171993911719939,
          0.12307692307692308,
          0.12232415902140673,
          0.12269938650306748
        ],
        "5": [
          0.1232876712328767,
          0.1171993911719939,
          0.11846153846153847,
          0.12385321100917432,
          0.1196319018404908
        ],
        "6": [
          0.136986301369863,
          0.15220700152207,
          0.11692307692307692,
          0.11773700305810397,
          0.1196319018404908
        ],
        "7": [
          0.1278538812785388,
          0.1171993911719939,
          0.13076923076923078,
          0.1529051987767584,
          0.10889570552147239
        ],
        "8": [
          0.1293759512937595,
          0.1628614916286149,
          0.15538461538461537,
          0.12385321100917432,
          0.12269938650306748
        ],
        "9": [
          0.1232876712328767,
          0.1324200913242009,
          0.12307692307692308,
          0.12232415902140673,
          0.12576687116564417
        ],
        "10": [
          0.1765601217656012,
          0.1400304414003044,
          0.12461538461538461,
          0.11314984709480122,
          0.13190184049079753
        ],
        "11": [
          0.1187214611872146,
          0.1232876712328767,
          0.11846153846153847,
          0.14220183486238533,
          0.147239263803681
        ],
        "12": [
          0.1491628614916286,
          0.1430745814307458,
          0.1723076923076923,
          0.13608562691131498,
          0.13190184049079753
        ],
        "13": [
          0.1567732115677321,
          0.1704718417047184,
          0.14,
          0.15749235474006115,
          0.16104294478527606
        ],
        "14": [
          0.1659056316590563,
          0.1719939117199391,
          0.12461538461538461,
          0.15902140672782875,
          0.16717791411042945
        ],
        "15": [
          0.182648401826484,
          0.1628614916286149,
          0.16615384615384615,
          0.18042813455657492,
          0.18558282208588958
        ],
        "16": [
          0.1735159817351598,
          0.1613394216133942,
          0.16615384615384615,
          0.19418960244648317,
          0.15644171779141106
        ],
        "17": [
          0.1963470319634703,
          0.1750380517503805,
          0.19230769230769232,
          0.17889908256880735,
          0.18404907975460122
        ],
        "18": [
          0.1811263318112633,
          0.1339421613394216,
          0.1676923076923077,
          0.19571865443425077,
          0.18865030674846625
        ],
        "19": [
          0.1430745814307458,
          0.1872146118721461,
          0.18461538461538463,
          0.1712538226299694,
          0.19478527607361965
        ],
        "20": [
          0.1293759512937595,
          0.1613394216133942,
          0.18307692307692308,
          0.18654434250764526,
          0.1579754601226994
        ],
        "21": [
          0.2024353120243531,
          0.1887366818873668,
          0.17384615384615384,
          0.17584097859327216,
          0.17177914110429449
        ],
        "22": [
          0.1704718417047184,
          0.1856925418569254,
          0.2,
          0.172782874617737,
          0.16257668711656442
        ],
        "23": [
          0.1765601217656012,
          0.1628614916286149,
          0.18769230769230769,
          0.1712538226299694,
          0.18711656441717792
        ],
        "24": [
          0.1719939117199391,
          0.1719939117199391,
          0.20615384615384616,
          0.19877675840978593,
          0.23006134969325154
        ],
        "25": [
          0.1765601217656012,
          0.1963470319634703,
          0.19076923076923077,
          0.19877675840978593,
          0.21012269938650308
        ],
        "26": [
          0.2207001522070015,
          0.2313546423135464,
          0.20307692307692307,
          0.17737003058103976,
          0.21625766871165644
        ],
        "27": [
          0.1948249619482496,
          0.1917808219178082,
          0.2815384615384615,
          0.20795107033639143,
          0.16104294478527606
        ],
        "28": [
          0.2237442922374429,
          0.2298325722983257,
          0.2523076923076923,
          0.2492354740061162,
          0.254601226993865
        ],
        "29": [
          0.2663622526636225,
          0.2648401826484018,
          0.26615384615384613,
          0.22324159021406728,
          0.2607361963190184
        ],
        "30": [
          0.2222222222222222,
          0.2496194824961948,
          0.19846153846153847,
          0.25229357798165136,
          0.2361963190184049
        ],
        "31": [
          0.2770167427701674,
          0.1963470319634703,
          0.2784615384615385,
          0.28287461773700306,
          0.2668711656441718
        ],
        "32": [
          0.2420091324200913,
          0.2207001522070015,
          0.24769230769230768,
          0.3302752293577982,
          0.2837423312883436
        ]
      },
      "best_layer_indices_reps": [
        31,
        29,
        27,
        32,
        32
      ],
      "best_layer_accuracies_reps": [
        0.2770167427701674,
        0.2648401826484018,
        0.2815384615384615,
        0.3302752293577982,
        0.2837423312883436
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.36      0.42      0.39        33\n           1       0.24      0.12      0.16        41\n           2       0.00      0.00      0.00        60\n           3       0.24      0.49      0.32        65\n           4       0.34      0.14      0.20        71\n           5       0.25      0.28      0.26        72\n           6       0.31      0.27      0.29        78\n           7       0.00      0.00      0.00        79\n           8       0.24      0.10      0.14        80\n           9       0.29      0.92      0.44        78\n\n    accuracy                           0.28       657\n   macro avg       0.23      0.27      0.22       657\nweighted avg       0.22      0.28      0.21       657\n",
        "              precision    recall  f1-score   support\n\n           0       0.22      0.85      0.35        34\n           1       0.42      0.11      0.18        44\n           2       0.18      0.03      0.06        60\n           3       0.00      0.00      0.00        65\n           4       0.00      0.00      0.00        69\n           5       0.21      0.36      0.26        74\n           6       0.46      0.16      0.23        77\n           7       0.26      0.65      0.37        77\n           8       0.28      0.38      0.32        80\n           9       0.47      0.25      0.32        77\n\n    accuracy                           0.26       657\n   macro avg       0.25      0.28      0.21       657\nweighted avg       0.25      0.26      0.21       657\n",
        "              precision    recall  f1-score   support\n\n           0       0.90      0.24      0.38        38\n           1       0.64      0.21      0.32        42\n           2       0.29      0.16      0.21        56\n           3       0.24      0.42      0.30        62\n           4       0.18      0.26      0.21        69\n           5       0.24      0.32      0.27        74\n           6       0.11      0.01      0.02        76\n           7       0.23      0.42      0.30        76\n           8       0.36      0.39      0.38        80\n           9       0.45      0.31      0.37        77\n\n    accuracy                           0.28       650\n   macro avg       0.37      0.27      0.28       650\nweighted avg       0.33      0.28      0.27       650\n",
        "              precision    recall  f1-score   support\n\n           0       0.89      0.43      0.58        37\n           1       0.22      0.51      0.31        39\n           2       0.00      0.00      0.00        60\n           3       0.24      0.63      0.35        65\n           4       0.37      0.23      0.29        69\n           5       0.27      0.23      0.25        75\n           6       0.50      0.01      0.03        77\n           7       0.00      0.00      0.00        77\n           8       0.45      0.46      0.45        80\n           9       0.36      0.91      0.52        75\n\n    accuracy                           0.33       654\n   macro avg       0.33      0.34      0.28       654\nweighted avg       0.31      0.33      0.26       654\n",
        "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        36\n           1       0.20      0.74      0.32        42\n           2       0.36      0.31      0.33        55\n           3       0.31      0.17      0.22        65\n           4       0.24      0.55      0.33        71\n           5       0.27      0.04      0.07        71\n           6       0.24      0.15      0.19        78\n           7       0.25      0.23      0.24        78\n           8       0.37      0.39      0.38        80\n           9       0.57      0.30      0.40        76\n\n    accuracy                           0.28       652\n   macro avg       0.28      0.29      0.25       652\nweighted avg       0.30      0.28      0.26       652\n"
      ]
    },
    "700-709": {
      "target_sums": [
        700,
        701,
        702,
        703,
        704,
        705,
        706,
        707,
        708,
        709
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.7075,
        0.712,
        0.70825,
        0.698,
        0.71225
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2830,
        2848,
        2833,
        2792,
        2849
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.13604240282685512,
          0.1368421052631579,
          0.13756613756613756,
          0.13953488372093023,
          0.13859649122807016
        ],
        "2": [
          0.15901060070671377,
          0.1368421052631579,
          0.13756613756613756,
          0.1413237924865832,
          0.13859649122807016
        ],
        "3": [
          0.13604240282685512,
          0.13508771929824562,
          0.13756613756613756,
          0.14311270125223613,
          0.13859649122807016
        ],
        "4": [
          0.13604240282685512,
          0.12456140350877193,
          0.13580246913580246,
          0.13774597495527727,
          0.13508771929824562
        ],
        "5": [
          0.13250883392226148,
          0.1368421052631579,
          0.14285714285714285,
          0.13953488372093023,
          0.13859649122807016
        ],
        "6": [
          0.13604240282685512,
          0.12982456140350876,
          0.13756613756613756,
          0.1413237924865832,
          0.13859649122807016
        ],
        "7": [
          0.13604240282685512,
          0.1368421052631579,
          0.13580246913580246,
          0.12880143112701253,
          0.14736842105263157
        ],
        "8": [
          0.1501766784452297,
          0.14736842105263157,
          0.15520282186948853,
          0.1592128801431127,
          0.13508771929824562
        ],
        "9": [
          0.13074204946996468,
          0.16842105263157894,
          0.17989417989417988,
          0.14311270125223613,
          0.1824561403508772
        ],
        "10": [
          0.14664310954063603,
          0.14385964912280702,
          0.13580246913580246,
          0.18246869409660108,
          0.1368421052631579
        ],
        "11": [
          0.15371024734982333,
          0.1368421052631579,
          0.13756613756613756,
          0.14669051878354203,
          0.14736842105263157
        ],
        "12": [
          0.1501766784452297,
          0.18421052631578946,
          0.18694885361552027,
          0.18783542039355994,
          0.15964912280701754
        ],
        "13": [
          0.19081272084805653,
          0.15964912280701754,
          0.19047619047619047,
          0.16994633273703041,
          0.18596491228070175
        ],
        "14": [
          0.19434628975265017,
          0.17894736842105263,
          0.14814814814814814,
          0.19499105545617174,
          0.156140350877193
        ],
        "15": [
          0.21201413427561838,
          0.18947368421052632,
          0.1710758377425044,
          0.18246869409660108,
          0.21228070175438596
        ],
        "16": [
          0.1872791519434629,
          0.18947368421052632,
          0.18342151675485008,
          0.18246869409660108,
          0.22982456140350876
        ],
        "17": [
          0.1519434628975265,
          0.21052631578947367,
          0.1746031746031746,
          0.25402504472271914,
          0.21228070175438596
        ],
        "18": [
          0.2314487632508834,
          0.19649122807017544,
          0.18694885361552027,
          0.22719141323792486,
          0.18070175438596492
        ],
        "19": [
          0.21024734982332155,
          0.20175438596491227,
          0.14814814814814814,
          0.19856887298747763,
          0.18421052631578946
        ],
        "20": [
          0.19081272084805653,
          0.19824561403508772,
          0.18871252204585537,
          0.21288014311270126,
          0.18947368421052632
        ],
        "21": [
          0.21201413427561838,
          0.19473684210526315,
          0.20282186948853614,
          0.22719141323792486,
          0.19649122807017544
        ],
        "22": [
          0.2332155477031802,
          0.18596491228070175,
          0.19929453262786595,
          0.24865831842576028,
          0.18421052631578946
        ],
        "23": [
          0.21731448763250882,
          0.2,
          0.2310405643738977,
          0.22182468694096602,
          0.23684210526315788
        ],
        "24": [
          0.2137809187279152,
          0.20701754385964913,
          0.19400352733686066,
          0.20214669051878353,
          0.23333333333333334
        ],
        "25": [
          0.17314487632508835,
          0.22280701754385965,
          0.2239858906525573,
          0.21288014311270126,
          0.23684210526315788
        ],
        "26": [
          0.2508833922261484,
          0.22631578947368422,
          0.26278659611992944,
          0.22003577817531306,
          0.19649122807017544
        ],
        "27": [
          0.24381625441696114,
          0.23859649122807017,
          0.21869488536155202,
          0.19320214669051877,
          0.2789473684210526
        ],
        "28": [
          0.24558303886925795,
          0.24035087719298245,
          0.2716049382716049,
          0.23434704830053668,
          0.2789473684210526
        ],
        "29": [
          0.2791519434628975,
          0.2543859649122807,
          0.2310405643738977,
          0.2504472271914132,
          0.2596491228070175
        ],
        "30": [
          0.2314487632508834,
          0.26666666666666666,
          0.26102292768959434,
          0.25402504472271914,
          0.2824561403508772
        ],
        "31": [
          0.24558303886925795,
          0.2719298245614035,
          0.2275132275132275,
          0.2898032200357782,
          0.3
        ],
        "32": [
          0.2756183745583039,
          0.32105263157894737,
          0.2716049382716049,
          0.33989266547406083,
          0.2824561403508772
        ]
      },
      "best_layer_indices_reps": [
        29,
        32,
        28,
        32,
        31
      ],
      "best_layer_accuracies_reps": [
        0.2791519434628975,
        0.32105263157894737,
        0.2716049382716049,
        0.33989266547406083,
        0.3
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       1.00      0.03      0.06        30\n           1       0.24      0.28      0.26        29\n           2       0.33      0.03      0.05        38\n           3       0.30      0.37      0.33        46\n           4       0.34      0.25      0.29        63\n           5       0.17      0.65      0.27        62\n           6       0.00      0.00      0.00        72\n           7       0.28      0.11      0.16        72\n           8       0.38      0.61      0.47        77\n           9       0.49      0.26      0.34        77\n\n    accuracy                           0.28       566\n   macro avg       0.35      0.26      0.22       566\nweighted avg       0.32      0.28      0.24       566\n",
        "              precision    recall  f1-score   support\n\n           0       0.46      0.24      0.32        25\n           1       0.20      0.04      0.06        28\n           2       0.20      0.55      0.30        40\n           3       0.48      0.22      0.30        50\n           4       0.30      0.42      0.35        64\n           5       0.00      0.00      0.00        64\n           6       0.23      0.79      0.35        71\n           7       0.71      0.14      0.23        73\n           8       1.00      0.04      0.07        78\n           9       0.69      0.61      0.65        77\n\n    accuracy                           0.32       570\n   macro avg       0.43      0.30      0.26       570\nweighted avg       0.47      0.32      0.27       570\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.08      0.15        25\n           1       0.25      0.76      0.38        29\n           2       0.40      0.09      0.15        44\n           3       1.00      0.02      0.04        46\n           4       0.29      0.38      0.33        63\n           5       0.20      0.17      0.18        63\n           6       0.16      0.09      0.11        70\n           7       0.00      0.00      0.00        72\n           8       0.38      0.15      0.22        78\n           9       0.28      0.94      0.44        77\n\n    accuracy                           0.27       567\n   macro avg       0.40      0.27      0.20       567\nweighted avg       0.33      0.27      0.20       567\n",
        "              precision    recall  f1-score   support\n\n           0       0.52      0.48      0.50        27\n           1       0.30      0.32      0.31        25\n           2       0.67      0.43      0.52        42\n           3       0.23      0.38      0.29        42\n           4       0.39      0.12      0.18        60\n           5       0.23      0.17      0.20        64\n           6       0.23      0.08      0.12        71\n           7       0.28      0.61      0.39        71\n           8       0.38      0.76      0.51        79\n           9       0.89      0.10      0.18        78\n\n    accuracy                           0.34       559\n   macro avg       0.41      0.35      0.32       559\nweighted avg       0.42      0.34      0.30       559\n",
        "              precision    recall  f1-score   support\n\n           0       0.30      0.46      0.37        28\n           1       0.00      0.00      0.00        26\n           2       0.25      0.45      0.32        40\n           3       0.00      0.00      0.00        47\n           4       0.00      0.00      0.00        62\n           5       0.31      0.37      0.34        65\n           6       0.23      0.40      0.29        73\n           7       0.00      0.00      0.00        73\n           8       0.49      0.23      0.31        79\n           9       0.33      0.90      0.48        77\n\n    accuracy                           0.30       570\n   macro avg       0.19      0.28      0.21       570\nweighted avg       0.21      0.30      0.22       570\n"
      ]
    },
    "800-809": {
      "target_sums": [
        800,
        801,
        802,
        803,
        804,
        805,
        806,
        807,
        808,
        809
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.70325,
        0.688,
        0.698,
        0.689,
        0.691
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2813,
        2752,
        2792,
        2756,
        2764
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.14031971580817051,
          0.14156079854809436,
          0.13774597495527727,
          0.14130434782608695,
          0.1410488245931284
        ],
        "2": [
          0.14031971580817051,
          0.14156079854809436,
          0.13953488372093023,
          0.14130434782608695,
          0.1410488245931284
        ],
        "3": [
          0.14031971580817051,
          0.14156079854809436,
          0.13953488372093023,
          0.14673913043478262,
          0.1410488245931284
        ],
        "4": [
          0.14031971580817051,
          0.14156079854809436,
          0.13953488372093023,
          0.14855072463768115,
          0.14647377938517178
        ],
        "5": [
          0.14564831261101244,
          0.14156079854809436,
          0.13595706618962433,
          0.1358695652173913,
          0.13562386980108498
        ],
        "6": [
          0.12966252220248667,
          0.14156079854809436,
          0.14311270125223613,
          0.14130434782608695,
          0.1410488245931284
        ],
        "7": [
          0.12966252220248667,
          0.15063520871143377,
          0.13774597495527727,
          0.18840579710144928,
          0.14466546112115733
        ],
        "8": [
          0.13854351687388988,
          0.1542649727767695,
          0.14669051878354203,
          0.1358695652173913,
          0.1410488245931284
        ],
        "9": [
          0.13854351687388988,
          0.13430127041742287,
          0.16457960644007155,
          0.15217391304347827,
          0.15913200723327306
        ],
        "10": [
          0.17939609236234458,
          0.15970961887477314,
          0.14311270125223613,
          0.1956521739130435,
          0.1518987341772152
        ],
        "11": [
          0.16518650088809947,
          0.19056261343012704,
          0.17173524150268335,
          0.1358695652173913,
          0.16636528028933092
        ],
        "12": [
          0.16341030195381884,
          0.17967332123411978,
          0.16636851520572452,
          0.1431159420289855,
          0.14647377938517178
        ],
        "13": [
          0.17229129662522202,
          0.19056261343012704,
          0.18962432915921287,
          0.1721014492753623,
          0.14647377938517178
        ],
        "14": [
          0.15630550621669628,
          0.18330308529945555,
          0.17889087656529518,
          0.19746376811594202,
          0.15009041591320071
        ],
        "15": [
          0.18294849023090587,
          0.1996370235934664,
          0.19141323792486584,
          0.20108695652173914,
          0.189873417721519
        ],
        "16": [
          0.19715808170515098,
          0.1996370235934664,
          0.19856887298747763,
          0.19927536231884058,
          0.1735985533453888
        ],
        "17": [
          0.1989342806394316,
          0.2177858439201452,
          0.19856887298747763,
          0.22826086956521738,
          0.20795660036166366
        ],
        "18": [
          0.20426287744227353,
          0.1851179673321234,
          0.1806797853309481,
          0.21014492753623187,
          0.1410488245931284
        ],
        "19": [
          0.15452930728241562,
          0.17604355716878403,
          0.19499105545617174,
          0.16847826086956522,
          0.19168173598553345
        ],
        "20": [
          0.16341030195381884,
          0.16878402903811252,
          0.16279069767441862,
          0.18659420289855072,
          0.19529837251356238
        ],
        "21": [
          0.19182948490230906,
          0.16696914700544466,
          0.18783542039355994,
          0.19927536231884058,
          0.19168173598553345
        ],
        "22": [
          0.21492007104795738,
          0.16696914700544466,
          0.2039355992844365,
          0.19021739130434784,
          0.21338155515370705
        ],
        "23": [
          0.1989342806394316,
          0.19600725952813067,
          0.2075134168157424,
          0.19202898550724637,
          0.17540687160940324
        ],
        "24": [
          0.16163410301953818,
          0.19237749546279492,
          0.18962432915921287,
          0.18659420289855072,
          0.15370705244122965
        ],
        "25": [
          0.20603907637655416,
          0.19782214156079855,
          0.22540250447227192,
          0.21014492753623187,
          0.23688969258589512
        ],
        "26": [
          0.22735346358792186,
          0.2159709618874773,
          0.18962432915921287,
          0.17391304347826086,
          0.19168173598553345
        ],
        "27": [
          0.19715808170515098,
          0.22323049001814882,
          0.19320214669051877,
          0.2807971014492754,
          0.2423146473779385
        ],
        "28": [
          0.23978685612788633,
          0.23593466424682397,
          0.26475849731663686,
          0.2572463768115942,
          0.28390596745027125
        ],
        "29": [
          0.21492007104795738,
          0.22141560798548093,
          0.22361359570661896,
          0.23550724637681159,
          0.24593128390596744
        ],
        "30": [
          0.2255772646536412,
          0.24500907441016334,
          0.24508050089445438,
          0.2717391304347826,
          0.27486437613019893
        ],
        "31": [
          0.261101243339254,
          0.23774954627949182,
          0.25402504472271914,
          0.21920289855072464,
          0.2730560578661845
        ],
        "32": [
          0.2344582593250444,
          0.27404718693284935,
          0.2737030411449016,
          0.2971014492753623,
          0.24593128390596744
        ]
      },
      "best_layer_indices_reps": [
        31,
        32,
        32,
        32,
        28
      ],
      "best_layer_accuracies_reps": [
        0.261101243339254,
        0.27404718693284935,
        0.2737030411449016,
        0.2971014492753623,
        0.28390596745027125
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.33      0.04      0.07        24\n           1       0.36      0.50      0.42        32\n           2       0.21      0.54      0.30        39\n           3       0.00      0.00      0.00        49\n           4       0.22      0.03      0.06        62\n           5       0.18      0.23      0.20        61\n           6       0.13      0.09      0.11        69\n           7       0.30      0.12      0.17        73\n           8       0.30      0.78      0.44        79\n           9       0.34      0.21      0.26        75\n\n    accuracy                           0.26       563\n   macro avg       0.24      0.26      0.20       563\nweighted avg       0.24      0.26      0.21       563\n",
        "              precision    recall  f1-score   support\n\n           0       0.25      0.92      0.40        25\n           1       0.00      0.00      0.00        28\n           2       0.00      0.00      0.00        37\n           3       0.29      0.46      0.35        46\n           4       0.19      0.29      0.23        59\n           5       0.16      0.22      0.19        60\n           6       0.29      0.14      0.19        69\n           7       0.62      0.07      0.12        74\n           8       0.35      0.74      0.48        78\n           9       0.44      0.05      0.10        75\n\n    accuracy                           0.27       551\n   macro avg       0.26      0.29      0.21       551\nweighted avg       0.30      0.27      0.21       551\n",
        "              precision    recall  f1-score   support\n\n           0       0.75      0.39      0.51        23\n           1       0.40      0.46      0.43        26\n           2       0.19      0.46      0.27        39\n           3       1.00      0.02      0.04        45\n           4       0.19      0.30      0.23        61\n           5       0.25      0.08      0.12        63\n           6       0.19      0.30      0.23        73\n           7       0.25      0.39      0.30        74\n           8       0.42      0.27      0.33        78\n           9       0.78      0.23      0.36        77\n\n    accuracy                           0.27       559\n   macro avg       0.44      0.29      0.28       559\nweighted avg       0.42      0.27      0.27       559\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.08      0.15        24\n           1       0.25      0.26      0.26        23\n           2       0.25      0.43      0.31        37\n           3       0.19      0.84      0.32        45\n           4       0.25      0.03      0.06        62\n           5       0.17      0.11      0.13        63\n           6       0.32      0.24      0.27        72\n           7       0.33      0.04      0.07        73\n           8       0.61      0.24      0.35        78\n           9       0.44      0.72      0.55        75\n\n    accuracy                           0.30       552\n   macro avg       0.38      0.30      0.25       552\nweighted avg       0.37      0.30      0.25       552\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.17      0.29        24\n           1       0.27      0.31      0.29        26\n           2       0.41      0.23      0.30        39\n           3       0.23      0.18      0.20        44\n           4       0.19      0.14      0.16        59\n           5       0.36      0.23      0.28        62\n           6       0.19      0.07      0.10        72\n           7       0.21      0.16      0.18        74\n           8       0.30      0.31      0.30        78\n           9       0.30      0.87      0.45        75\n\n    accuracy                           0.28       553\n   macro avg       0.34      0.27      0.25       553\nweighted avg       0.30      0.28      0.25       553\n"
      ]
    },
    "900-909": {
      "target_sums": [
        900,
        901,
        902,
        903,
        904,
        905,
        906,
        907,
        908,
        909
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.63175,
        0.61325,
        0.6095,
        0.61975,
        0.61
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2527,
        2453,
        2438,
        2479,
        2440
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.15217391304347827,
          0.15274949083503056,
          0.1557377049180328,
          0.15524193548387097,
          0.15368852459016394
        ],
        "2": [
          0.1482213438735178,
          0.15682281059063136,
          0.16598360655737704,
          0.15725806451612903,
          0.15368852459016394
        ],
        "3": [
          0.15217391304347827,
          0.15274949083503056,
          0.1557377049180328,
          0.15524193548387097,
          0.1557377049180328
        ],
        "4": [
          0.13636363636363635,
          0.15274949083503056,
          0.1557377049180328,
          0.1532258064516129,
          0.1557377049180328
        ],
        "5": [
          0.15019762845849802,
          0.15682281059063136,
          0.1557377049180328,
          0.15524193548387097,
          0.15368852459016394
        ],
        "6": [
          0.16205533596837945,
          0.15682281059063136,
          0.1557377049180328,
          0.15524193548387097,
          0.1557377049180328
        ],
        "7": [
          0.1482213438735178,
          0.15682281059063136,
          0.1557377049180328,
          0.1532258064516129,
          0.15368852459016394
        ],
        "8": [
          0.18774703557312253,
          0.15682281059063136,
          0.19672131147540983,
          0.15120967741935484,
          0.1557377049180328
        ],
        "9": [
          0.2134387351778656,
          0.15682281059063136,
          0.1557377049180328,
          0.19556451612903225,
          0.1762295081967213
        ],
        "10": [
          0.1324110671936759,
          0.15274949083503056,
          0.20901639344262296,
          0.17943548387096775,
          0.17827868852459017
        ],
        "11": [
          0.15612648221343872,
          0.1670061099796334,
          0.1557377049180328,
          0.15524193548387097,
          0.22540983606557377
        ],
        "12": [
          0.15019762845849802,
          0.16089613034623218,
          0.2151639344262295,
          0.16129032258064516,
          0.21106557377049182
        ],
        "13": [
          0.191699604743083,
          0.17311608961303462,
          0.16598360655737704,
          0.16129032258064516,
          0.2069672131147541
        ],
        "14": [
          0.17786561264822134,
          0.17515274949083504,
          0.22336065573770492,
          0.15524193548387097,
          0.21721311475409835
        ],
        "15": [
          0.2015810276679842,
          0.21792260692464357,
          0.23975409836065573,
          0.19959677419354838,
          0.19467213114754098
        ],
        "16": [
          0.18379446640316205,
          0.219959266802444,
          0.2520491803278688,
          0.19959677419354838,
          0.21721311475409835
        ],
        "17": [
          0.18379446640316205,
          0.20977596741344195,
          0.2540983606557377,
          0.2056451612903226,
          0.24180327868852458
        ],
        "18": [
          0.15612648221343872,
          0.20162932790224034,
          0.22950819672131148,
          0.22782258064516128,
          0.20081967213114754
        ],
        "19": [
          0.17984189723320157,
          0.1975560081466395,
          0.22131147540983606,
          0.21370967741935484,
          0.15778688524590165
        ],
        "20": [
          0.18379446640316205,
          0.1955193482688391,
          0.2151639344262295,
          0.1975806451612903,
          0.17008196721311475
        ],
        "21": [
          0.19960474308300397,
          0.1955193482688391,
          0.1721311475409836,
          0.2701612903225806,
          0.19672131147540983
        ],
        "22": [
          0.1857707509881423,
          0.1894093686354379,
          0.22131147540983606,
          0.22379032258064516,
          0.22336065573770492
        ],
        "23": [
          0.19960474308300397,
          0.18737270875763748,
          0.22336065573770492,
          0.25806451612903225,
          0.16188524590163936
        ],
        "24": [
          0.1857707509881423,
          0.20773930753564154,
          0.18442622950819673,
          0.2439516129032258,
          0.1987704918032787
        ],
        "25": [
          0.22134387351778656,
          0.18329938900203666,
          0.1762295081967213,
          0.2318548387096774,
          0.1885245901639344
        ],
        "26": [
          0.21146245059288538,
          0.17922606924643583,
          0.2520491803278688,
          0.27419354838709675,
          0.18442622950819673
        ],
        "27": [
          0.23517786561264822,
          0.21384928716904278,
          0.22336065573770492,
          0.2056451612903226,
          0.21721311475409835
        ],
        "28": [
          0.25296442687747034,
          0.2484725050916497,
          0.23565573770491804,
          0.22580645161290322,
          0.22540983606557377
        ],
        "29": [
          0.25296442687747034,
          0.2708757637474542,
          0.26024590163934425,
          0.24596774193548387,
          0.23770491803278687
        ],
        "30": [
          0.26679841897233203,
          0.2484725050916497,
          0.22540983606557377,
          0.2600806451612903,
          0.2192622950819672
        ],
        "31": [
          0.233201581027668,
          0.26476578411405294,
          0.2520491803278688,
          0.23790322580645162,
          0.21721311475409835
        ],
        "32": [
          0.25889328063241107,
          0.20977596741344195,
          0.23770491803278687,
          0.2701612903225806,
          0.20491803278688525
        ]
      },
      "best_layer_indices_reps": [
        30,
        29,
        29,
        26,
        17
      ],
      "best_layer_accuracies_reps": [
        0.26679841897233203,
        0.2708757637474542,
        0.26024590163934425,
        0.27419354838709675,
        0.24180327868852458
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.29      0.64      0.39        22\n           1       0.00      0.00      0.00        22\n           2       0.00      0.00      0.00        31\n           3       1.00      0.03      0.05        36\n           4       0.19      0.31      0.24        51\n           5       0.22      0.32      0.26        56\n           6       0.34      0.32      0.33        66\n           7       0.21      0.16      0.18        70\n           8       0.17      0.01      0.02        75\n           9       0.31      0.69      0.43        77\n\n    accuracy                           0.27       506\n   macro avg       0.27      0.25      0.19       506\nweighted avg       0.27      0.27      0.21       506\n",
        "              precision    recall  f1-score   support\n\n           0       0.56      0.26      0.36        19\n           1       1.00      0.05      0.09        21\n           2       0.00      0.00      0.00        29\n           3       0.19      0.43      0.26        37\n           4       0.20      0.02      0.04        47\n           5       0.41      0.13      0.20        54\n           6       0.00      0.00      0.00        63\n           7       0.22      0.68      0.34        69\n           8       0.33      0.59      0.42        75\n           9       0.43      0.16      0.23        77\n\n    accuracy                           0.27       491\n   macro avg       0.33      0.23      0.19       491\nweighted avg       0.29      0.27      0.21       491\n",
        "              precision    recall  f1-score   support\n\n           0       0.33      0.35      0.34        17\n           1       0.00      0.00      0.00        22\n           2       0.50      0.07      0.13        27\n           3       0.35      0.32      0.34        37\n           4       0.50      0.02      0.04        46\n           5       0.33      0.02      0.04        54\n           6       0.14      0.25      0.18        64\n           7       0.21      0.22      0.21        69\n           8       0.28      0.67      0.39        76\n           9       0.41      0.30      0.35        76\n\n    accuracy                           0.26       488\n   macro avg       0.31      0.22      0.20       488\nweighted avg       0.30      0.26      0.22       488\n",
        "              precision    recall  f1-score   support\n\n           0       0.42      0.26      0.32        19\n           1       0.15      0.14      0.14        22\n           2       1.00      0.03      0.06        32\n           3       0.23      0.38      0.29        37\n           4       0.07      0.02      0.03        46\n           5       0.17      0.04      0.06        52\n           6       0.22      0.18      0.20        67\n           7       0.20      0.29      0.24        68\n           8       0.35      0.34      0.34        76\n           9       0.35      0.68      0.46        77\n\n    accuracy                           0.27       496\n   macro avg       0.32      0.24      0.22       496\nweighted avg       0.29      0.27      0.24       496\n",
        "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        18\n           1       0.00      0.00      0.00        23\n           2       0.00      0.00      0.00        31\n           3       0.00      0.00      0.00        33\n           4       0.13      0.11      0.12        46\n           5       0.16      0.31      0.21        52\n           6       0.13      0.12      0.12        66\n           7       0.00      0.00      0.00        68\n           8       0.37      0.56      0.44        75\n           9       0.27      0.62      0.38        76\n\n    accuracy                           0.24       488\n   macro avg       0.11      0.17      0.13       488\nweighted avg       0.15      0.24      0.18       488\n"
      ]
    }
  },
  "best_layer_mode_per_range": {
    "500-509": 32,
    "600-609": 32,
    "700-709": 32,
    "800-809": 32,
    "900-909": 29
  }
}