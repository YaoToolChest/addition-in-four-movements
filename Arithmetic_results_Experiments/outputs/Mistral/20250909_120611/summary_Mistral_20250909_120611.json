{
  "model": "/root/autodl-tmp/Mistral",
  "model_short": "Mistral",
  "timestamp": "20250909_120611",
  "device": "cuda:0",
  "hyper_params": {
    "MODEL_NAMES": [
      "/root/autodl-tmp/Mistral"
    ],
    "N_REPETITIONS": 5,
    "BASE_SEED": 42,
    "PROBE_TYPE": "linear",
    "PROBE_MLP_HIDDEN_DIM": 4096,
    "N_PROBE_EPOCHS": 20,
    "PROBE_LR": 0.001,
    "PROBE_BATCH_SIZE": 64,
    "TEST_SPLIT_RATIO": 0.2,
    "GENERATION_BATCH_SIZE": 500,
    "GEN_MAX_NEW_TOKENS": 3,
    "GEN_DO_SAMPLE": false,
    "TOK_MAX_LENGTH": 60,
    "TOK_PADDING_SIDE": "left",
    "SUM_RANGES_CONFIG": [
      {
        "label": "500-509",
        "start": 500,
        "count": 10
      },
      {
        "label": "600-609",
        "start": 600,
        "count": 10
      },
      {
        "label": "700-709",
        "start": 700,
        "count": 10
      },
      {
        "label": "800-809",
        "start": 800,
        "count": 10
      },
      {
        "label": "900-909",
        "start": 900,
        "count": 10
      }
    ],
    "N_SAMPLES_PER_SUM": 400,
    "MIN_SAMPLES_PER_CLASS_FOR_PROBING": 5,
    "MIN_ADDEND_VAL": 100,
    "A_BOUND_LOW_FRAC": 0.25,
    "A_BOUND_HIGH_FRAC": 0.75,
    "MAX_ATTEMPTS_MULTIPLIER": 20,
    "ACTIVATIONS_BATCH_SIZE": 256,
    "LOG_TO_CONSOLE": false,
    "TQDM_DISABLED": true,
    "SUPPRESS_WARNINGS": true
  },
  "probe_type": "linear",
  "probe_epochs": 20,
  "probe_lr": 0.001,
  "probe_batch_size": 64,
  "generation_batch_size": 500,
  "sum_ranges_config": [
    {
      "label": "500-509",
      "start": 500,
      "count": 10
    },
    {
      "label": "600-609",
      "start": 600,
      "count": 10
    },
    {
      "label": "700-709",
      "start": 700,
      "count": 10
    },
    {
      "label": "800-809",
      "start": 800,
      "count": 10
    },
    {
      "label": "900-909",
      "start": 900,
      "count": 10
    }
  ],
  "n_samples_per_sum": 400,
  "test_split_ratio": 0.2,
  "layers_to_probe": [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32
  ],
  "logical_num_layers": 32,
  "hidden_states_tuple_len": 33,
  "results": {
    "500-509": {
      "target_sums": [
        500,
        501,
        502,
        503,
        504,
        505,
        506,
        507,
        508,
        509
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.67525,
        0.665,
        0.6775,
        0.6815,
        0.6765
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2701,
        2660,
        2710,
        2726,
        2706
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.1367837338262477,
          0.15977443609022557,
          0.14206642066420663,
          0.14102564102564102,
          0.14391143911439114
        ],
        "2": [
          0.1423290203327172,
          0.14285714285714285,
          0.14206642066420663,
          0.14652014652014653,
          0.14391143911439114
        ],
        "3": [
          0.1367837338262477,
          0.14285714285714285,
          0.13468634686346864,
          0.14102564102564102,
          0.15682656826568267
        ],
        "4": [
          0.1423290203327172,
          0.15037593984962405,
          0.14206642066420663,
          0.1446886446886447,
          0.17343173431734318
        ],
        "5": [
          0.1478743068391867,
          0.14473684210526316,
          0.14391143911439114,
          0.14652014652014653,
          0.13468634686346864
        ],
        "6": [
          0.17375231053604437,
          0.16729323308270677,
          0.15682656826568267,
          0.19597069597069597,
          0.16051660516605165
        ],
        "7": [
          0.14602587800369685,
          0.15601503759398497,
          0.12730627306273062,
          0.1575091575091575,
          0.14944649446494465
        ],
        "8": [
          0.16266173752310537,
          0.13909774436090225,
          0.16420664206642066,
          0.184981684981685,
          0.2011070110701107
        ],
        "9": [
          0.21626617375231053,
          0.18045112781954886,
          0.1881918819188192,
          0.21611721611721613,
          0.19557195571955718
        ],
        "10": [
          0.15157116451016636,
          0.16729323308270677,
          0.14022140221402213,
          0.21794871794871795,
          0.19557195571955718
        ],
        "11": [
          0.1756007393715342,
          0.18984962406015038,
          0.18450184501845018,
          0.19963369963369965,
          0.1752767527675277
        ],
        "12": [
          0.1977818853974122,
          0.20864661654135339,
          0.16051660516605165,
          0.24725274725274726,
          0.2047970479704797
        ],
        "13": [
          0.21256931608133087,
          0.20300751879699247,
          0.21955719557195572,
          0.19413919413919414,
          0.23800738007380073
        ],
        "14": [
          0.20702402957486138,
          0.18984962406015038,
          0.17712177121771217,
          0.22344322344322345,
          0.21955719557195572
        ],
        "15": [
          0.24584103512014788,
          0.2387218045112782,
          0.2177121771217712,
          0.21978021978021978,
          0.2785977859778598
        ],
        "16": [
          0.26247689463955637,
          0.2462406015037594,
          0.21955719557195572,
          0.2490842490842491,
          0.24169741697416974
        ],
        "17": [
          0.28650646950092423,
          0.25,
          0.26383763837638374,
          0.2600732600732601,
          0.25830258302583026
        ],
        "18": [
          0.25693160813308685,
          0.24060150375939848,
          0.23247232472324722,
          0.24725274725274726,
          0.25830258302583026
        ],
        "19": [
          0.3031423290203327,
          0.2349624060150376,
          0.2177121771217712,
          0.2619047619047619,
          0.28044280442804426
        ],
        "20": [
          0.2846580406654344,
          0.2387218045112782,
          0.26199261992619927,
          0.21794871794871795,
          0.2656826568265683
        ],
        "21": [
          0.2920517560073937,
          0.27631578947368424,
          0.24907749077490776,
          0.27289377289377287,
          0.2785977859778598
        ],
        "22": [
          0.3197781885397412,
          0.2706766917293233,
          0.25276752767527677,
          0.2838827838827839,
          0.3044280442804428
        ],
        "23": [
          0.3659889094269871,
          0.2706766917293233,
          0.3044280442804428,
          0.28205128205128205,
          0.33210332103321033
        ],
        "24": [
          0.36044362292051757,
          0.3458646616541353,
          0.34501845018450183,
          0.3021978021978022,
          0.34686346863468637
        ],
        "25": [
          0.3197781885397412,
          0.3609022556390977,
          0.3726937269372694,
          0.2948717948717949,
          0.2988929889298893
        ],
        "26": [
          0.35489833641404805,
          0.3082706766917293,
          0.3413284132841328,
          0.304029304029304,
          0.36531365313653136
        ],
        "27": [
          0.3955637707948244,
          0.32894736842105265,
          0.3856088560885609,
          0.32051282051282054,
          0.3726937269372694
        ],
        "28": [
          0.36414048059149723,
          0.3966165413533835,
          0.36531365313653136,
          0.3663003663003663,
          0.4114391143911439
        ],
        "29": [
          0.4066543438077634,
          0.38533834586466165,
          0.3948339483394834,
          0.3333333333333333,
          0.35239852398523985
        ],
        "30": [
          0.4048059149722736,
          0.36278195488721804,
          0.4280442804428044,
          0.3772893772893773,
          0.4132841328413284
        ],
        "31": [
          0.4399260628465804,
          0.37406015037593987,
          0.37084870848708484,
          0.3974358974358974,
          0.3856088560885609
        ],
        "32": [
          0.4491682070240296,
          0.4680451127819549,
          0.45387453874538747,
          0.28205128205128205,
          0.33948339483394835
        ]
      },
      "best_layer_indices_reps": [
        32,
        32,
        32,
        31,
        30
      ],
      "best_layer_accuracies_reps": [
        0.4491682070240296,
        0.4680451127819549,
        0.45387453874538747,
        0.3974358974358974,
        0.4132841328413284
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       1.00      0.21      0.34        24\n           1       0.28      0.84      0.42        25\n           2       0.52      0.30      0.38        40\n           3       0.80      0.09      0.17        43\n           4       0.44      0.63      0.52        57\n           5       0.50      0.50      0.50        60\n           6       0.43      0.09      0.14        69\n           7       0.44      0.33      0.38        70\n           8       0.37      0.83      0.51        77\n           9       0.84      0.55      0.67        76\n\n    accuracy                           0.45       541\n   macro avg       0.56      0.44      0.40       541\nweighted avg       0.54      0.45      0.42       541\n",
        "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        20\n           1       0.57      0.18      0.28        22\n           2       0.34      0.44      0.38        34\n           3       0.34      0.75      0.47        44\n           4       0.42      0.77      0.55        62\n           5       0.80      0.07      0.12        60\n           6       0.00      0.00      0.00        69\n           7       0.56      0.42      0.48        69\n           8       0.50      0.80      0.62        76\n           9       0.59      0.72      0.65        76\n\n    accuracy                           0.47       532\n   macro avg       0.41      0.42      0.35       532\nweighted avg       0.44      0.47      0.40       532\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.43      0.61        23\n           1       0.34      0.54      0.42        28\n           2       0.28      0.64      0.39        36\n           3       0.00      0.00      0.00        43\n           4       0.44      0.34      0.39        58\n           5       0.39      0.79      0.52        62\n           6       0.58      0.21      0.31        70\n           7       0.53      0.26      0.35        69\n           8       0.50      0.91      0.64        77\n           9       0.76      0.34      0.47        76\n\n    accuracy                           0.45       542\n   macro avg       0.48      0.45      0.41       542\nweighted avg       0.49      0.45      0.42       542\n",
        "              precision    recall  f1-score   support\n\n           0       0.43      0.63      0.51        19\n           1       0.39      0.24      0.30        29\n           2       0.12      0.03      0.05        34\n           3       0.37      0.50      0.42        46\n           4       0.31      0.36      0.34        61\n           5       0.44      0.17      0.25        64\n           6       0.39      0.10      0.16        70\n           7       0.29      0.62      0.40        72\n           8       0.45      0.55      0.49        77\n           9       0.68      0.64      0.66        74\n\n    accuracy                           0.40       546\n   macro avg       0.39      0.38      0.36       546\nweighted avg       0.40      0.40      0.37       546\n",
        "              precision    recall  f1-score   support\n\n           0       0.50      0.55      0.52        22\n           1       0.42      0.38      0.40        26\n           2       0.44      0.42      0.43        38\n           3       0.36      0.30      0.33        44\n           4       0.43      0.10      0.16        59\n           5       0.60      0.34      0.44        61\n           6       0.24      0.84      0.37        68\n           7       0.00      0.00      0.00        71\n           8       0.65      0.45      0.53        78\n           9       0.69      0.72      0.71        75\n\n    accuracy                           0.41       542\n   macro avg       0.43      0.41      0.39       542\nweighted avg       0.43      0.41      0.38       542\n"
      ]
    },
    "600-609": {
      "target_sums": [
        600,
        601,
        602,
        603,
        604,
        605,
        606,
        607,
        608,
        609
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.82125,
        0.821,
        0.8115,
        0.8175,
        0.8145
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        3285,
        3284,
        3246,
        3270,
        3258
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.1202435312024353,
          0.1293759512937595,
          0.12307692307692308,
          0.12079510703363915,
          0.1165644171779141
        ],
        "2": [
          0.1141552511415525,
          0.1202435312024353,
          0.12615384615384614,
          0.12079510703363915,
          0.1165644171779141
        ],
        "3": [
          0.1171993911719939,
          0.1248097412480974,
          0.12153846153846154,
          0.13761467889908258,
          0.11196319018404909
        ],
        "4": [
          0.1293759512937595,
          0.1263318112633181,
          0.11692307692307692,
          0.12232415902140673,
          0.13190184049079753
        ],
        "5": [
          0.1156773211567732,
          0.1400304414003044,
          0.1123076923076923,
          0.12691131498470948,
          0.11809815950920245
        ],
        "6": [
          0.1248097412480974,
          0.1308980213089802,
          0.12461538461538461,
          0.14067278287461774,
          0.1196319018404908
        ],
        "7": [
          0.1308980213089802,
          0.1415525114155251,
          0.13230769230769232,
          0.12844036697247707,
          0.12116564417177914
        ],
        "8": [
          0.1796042617960426,
          0.1445966514459665,
          0.1723076923076923,
          0.17584097859327216,
          0.16257668711656442
        ],
        "9": [
          0.197869101978691,
          0.1628614916286149,
          0.18,
          0.17889908256880735,
          0.13957055214723926
        ],
        "10": [
          0.1719939117199391,
          0.1689497716894977,
          0.14153846153846153,
          0.17584097859327216,
          0.15644171779141106
        ],
        "11": [
          0.1856925418569254,
          0.1567732115677321,
          0.13692307692307693,
          0.1712538226299694,
          0.17177914110429449
        ],
        "12": [
          0.1993911719939117,
          0.1598173515981735,
          0.17076923076923076,
          0.20030581039755352,
          0.17177914110429449
        ],
        "13": [
          0.1993911719939117,
          0.167427701674277,
          0.18,
          0.19724770642201836,
          0.19171779141104295
        ],
        "14": [
          0.1750380517503805,
          0.1780821917808219,
          0.18,
          0.18807339449541285,
          0.18558282208588958
        ],
        "15": [
          0.2313546423135464,
          0.1963470319634703,
          0.19384615384615383,
          0.22324159021406728,
          0.2085889570552147
        ],
        "16": [
          0.2237442922374429,
          0.2207001522070015,
          0.2276923076923077,
          0.21559633027522937,
          0.21779141104294478
        ],
        "17": [
          0.2557077625570776,
          0.2313546423135464,
          0.2153846153846154,
          0.22477064220183487,
          0.24386503067484663
        ],
        "18": [
          0.2191780821917808,
          0.2207001522070015,
          0.24153846153846154,
          0.22324159021406728,
          0.2331288343558282
        ],
        "19": [
          0.2343987823439878,
          0.213089802130898,
          0.2123076923076923,
          0.23547400611620795,
          0.2331288343558282
        ],
        "20": [
          0.2298325722983257,
          0.2161339421613394,
          0.22615384615384615,
          0.21406727828746178,
          0.24846625766871167
        ],
        "21": [
          0.3089802130898021,
          0.2526636225266362,
          0.26615384615384613,
          0.2966360856269113,
          0.2469325153374233
        ],
        "22": [
          0.2526636225266362,
          0.2404870624048706,
          0.23846153846153847,
          0.28440366972477066,
          0.24386503067484663
        ],
        "23": [
          0.3181126331811263,
          0.2785388127853881,
          0.2923076923076923,
          0.3241590214067278,
          0.2745398773006135
        ],
        "24": [
          0.2952815829528158,
          0.3013698630136986,
          0.30923076923076925,
          0.3149847094801223,
          0.2898773006134969
        ],
        "25": [
          0.3592085235920852,
          0.2511415525114155,
          0.3,
          0.3363914373088685,
          0.2668711656441718
        ],
        "26": [
          0.3226788432267884,
          0.2968036529680365,
          0.31846153846153846,
          0.35015290519877673,
          0.2837423312883436
        ],
        "27": [
          0.3257229832572298,
          0.3333333333333333,
          0.32461538461538464,
          0.3165137614678899,
          0.3558282208588957
        ],
        "28": [
          0.3242009132420091,
          0.3561643835616438,
          0.33692307692307694,
          0.3761467889908257,
          0.30214723926380366
        ],
        "29": [
          0.3318112633181126,
          0.3439878234398782,
          0.36153846153846153,
          0.3440366972477064,
          0.3328220858895706
        ],
        "30": [
          0.2907153729071537,
          0.3485540334855403,
          0.3569230769230769,
          0.35932721712538224,
          0.34049079754601225
        ],
        "31": [
          0.2922374429223744,
          0.3683409436834094,
          0.3923076923076923,
          0.3486238532110092,
          0.3496932515337423
        ],
        "32": [
          0.3607305936073059,
          0.3835616438356164,
          0.4323076923076923,
          0.41131498470948014,
          0.3911042944785276
        ]
      },
      "best_layer_indices_reps": [
        32,
        32,
        32,
        32,
        32
      ],
      "best_layer_accuracies_reps": [
        0.3607305936073059,
        0.3835616438356164,
        0.4323076923076923,
        0.41131498470948014,
        0.3911042944785276
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.76      0.48      0.59        33\n           1       0.32      0.90      0.47        41\n           2       0.50      0.05      0.09        60\n           3       0.00      0.00      0.00        65\n           4       0.34      0.70      0.46        71\n           5       0.63      0.17      0.26        72\n           6       0.75      0.08      0.14        78\n           7       0.29      0.56      0.38        79\n           8       0.45      0.06      0.11        80\n           9       0.36      0.82      0.50        78\n\n    accuracy                           0.36       657\n   macro avg       0.44      0.38      0.30       657\nweighted avg       0.43      0.36      0.28       657\n",
        "              precision    recall  f1-score   support\n\n           0       0.80      0.59      0.68        34\n           1       0.83      0.34      0.48        44\n           2       0.44      0.70      0.54        60\n           3       0.30      0.78      0.43        65\n           4       0.50      0.07      0.13        69\n           5       0.67      0.03      0.05        74\n           6       0.67      0.03      0.05        77\n           7       0.28      0.78      0.42        77\n           8       0.46      0.69      0.55        80\n           9       0.00      0.00      0.00        77\n\n    accuracy                           0.38       657\n   macro avg       0.49      0.40      0.33       657\nweighted avg       0.46      0.38      0.30       657\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.47      0.64        38\n           1       0.79      0.52      0.63        42\n           2       0.52      0.59      0.55        56\n           3       0.45      0.27      0.34        62\n           4       0.38      0.39      0.38        69\n           5       0.38      0.20      0.26        74\n           6       0.59      0.22      0.32        76\n           7       0.25      0.82      0.38        76\n           8       0.66      0.41      0.51        80\n           9       0.59      0.48      0.53        77\n\n    accuracy                           0.43       650\n   macro avg       0.56      0.44      0.46       650\nweighted avg       0.53      0.43      0.44       650\n",
        "              precision    recall  f1-score   support\n\n           0       0.75      0.24      0.37        37\n           1       0.75      0.46      0.57        39\n           2       0.35      0.65      0.45        60\n           3       0.00      0.00      0.00        65\n           4       0.31      0.71      0.43        69\n           5       0.44      0.09      0.15        75\n           6       0.53      0.12      0.19        77\n           7       0.30      0.71      0.43        77\n           8       0.84      0.26      0.40        80\n           9       0.57      0.83      0.67        75\n\n    accuracy                           0.41       654\n   macro avg       0.48      0.41      0.37       654\nweighted avg       0.47      0.41      0.36       654\n",
        "              precision    recall  f1-score   support\n\n           0       0.76      0.72      0.74        36\n           1       0.75      0.21      0.33        42\n           2       0.32      0.87      0.47        55\n           3       0.60      0.38      0.47        65\n           4       0.00      0.00      0.00        71\n           5       0.36      0.49      0.41        71\n           6       0.50      0.06      0.11        78\n           7       0.27      0.79      0.41        78\n           8       0.78      0.17      0.29        80\n           9       0.48      0.41      0.44        76\n\n    accuracy                           0.39       652\n   macro avg       0.48      0.41      0.37       652\nweighted avg       0.46      0.39      0.34       652\n"
      ]
    },
    "700-709": {
      "target_sums": [
        700,
        701,
        702,
        703,
        704,
        705,
        706,
        707,
        708,
        709
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.7075,
        0.712,
        0.70825,
        0.698,
        0.71225
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2830,
        2848,
        2833,
        2792,
        2849
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.13604240282685512,
          0.1368421052631579,
          0.13756613756613756,
          0.1520572450805009,
          0.13157894736842105
        ],
        "2": [
          0.15547703180212014,
          0.1456140350877193,
          0.13580246913580246,
          0.14669051878354203,
          0.13508771929824562
        ],
        "3": [
          0.13780918727915195,
          0.1368421052631579,
          0.11816578483245149,
          0.16100178890876565,
          0.12280701754385964
        ],
        "4": [
          0.1501766784452297,
          0.14035087719298245,
          0.13580246913580246,
          0.14669051878354203,
          0.14912280701754385
        ],
        "5": [
          0.1431095406360424,
          0.12631578947368421,
          0.14814814814814814,
          0.1449016100178891,
          0.14035087719298245
        ],
        "6": [
          0.1413427561837456,
          0.14912280701754385,
          0.14814814814814814,
          0.1449016100178891,
          0.14736842105263157
        ],
        "7": [
          0.16607773851590105,
          0.15789473684210525,
          0.15873015873015872,
          0.16457960644007155,
          0.1456140350877193
        ],
        "8": [
          0.16784452296819788,
          0.15263157894736842,
          0.1710758377425044,
          0.17531305903398928,
          0.18070175438596492
        ],
        "9": [
          0.19434628975265017,
          0.18070175438596492,
          0.164021164021164,
          0.18783542039355994,
          0.1912280701754386
        ],
        "10": [
          0.19081272084805653,
          0.16666666666666666,
          0.21164021164021163,
          0.2003577817531306,
          0.21403508771929824
        ],
        "11": [
          0.19434628975265017,
          0.19649122807017544,
          0.1746031746031746,
          0.20214669051878353,
          0.19824561403508772
        ],
        "12": [
          0.2067137809187279,
          0.20350877192982456,
          0.21693121693121692,
          0.22361359570661896,
          0.24210526315789474
        ],
        "13": [
          0.21201413427561838,
          0.18947368421052632,
          0.20105820105820105,
          0.22003577817531306,
          0.20877192982456141
        ],
        "14": [
          0.21731448763250882,
          0.21052631578947367,
          0.20105820105820105,
          0.2075134168157424,
          0.22280701754385965
        ],
        "15": [
          0.22614840989399293,
          0.22456140350877193,
          0.24514991181657847,
          0.23076923076923078,
          0.25263157894736843
        ],
        "16": [
          0.23498233215547704,
          0.24385964912280703,
          0.2239858906525573,
          0.24865831842576028,
          0.2596491228070175
        ],
        "17": [
          0.2579505300353357,
          0.24912280701754386,
          0.25396825396825395,
          0.23076923076923078,
          0.2894736842105263
        ],
        "18": [
          0.24911660777385158,
          0.22456140350877193,
          0.2222222222222222,
          0.27549194991055453,
          0.2754385964912281
        ],
        "19": [
          0.26855123674911663,
          0.24385964912280703,
          0.2275132275132275,
          0.2558139534883721,
          0.2824561403508772
        ],
        "20": [
          0.2332155477031802,
          0.256140350877193,
          0.19400352733686066,
          0.2772808586762075,
          0.2736842105263158
        ],
        "21": [
          0.3003533568904594,
          0.28421052631578947,
          0.2768959435626102,
          0.295169946332737,
          0.312280701754386
        ],
        "22": [
          0.303886925795053,
          0.2614035087719298,
          0.31746031746031744,
          0.28622540250447226,
          0.3298245614035088
        ],
        "23": [
          0.2950530035335689,
          0.2807017543859649,
          0.291005291005291,
          0.3363148479427549,
          0.3
        ],
        "24": [
          0.30565371024734983,
          0.32105263157894737,
          0.32451499118165783,
          0.334525939177102,
          0.30701754385964913
        ],
        "25": [
          0.3303886925795053,
          0.2964912280701754,
          0.328042328042328,
          0.26654740608228983,
          0.3368421052631579
        ],
        "26": [
          0.3321554770318021,
          0.3157894736842105,
          0.3421516754850088,
          0.3470483005366726,
          0.30350877192982456
        ],
        "27": [
          0.3498233215547703,
          0.35789473684210527,
          0.328042328042328,
          0.3291592128801431,
          0.37543859649122807
        ],
        "28": [
          0.36042402826855124,
          0.3526315789473684,
          0.32275132275132273,
          0.3148479427549195,
          0.3912280701754386
        ],
        "29": [
          0.3409893992932862,
          0.32280701754385965,
          0.3915343915343915,
          0.33810375670840787,
          0.3368421052631579
        ],
        "30": [
          0.38869257950530034,
          0.2824561403508772,
          0.3403880070546737,
          0.37924865831842575,
          0.37543859649122807
        ],
        "31": [
          0.3710247349823322,
          0.3736842105263158,
          0.37213403880070545,
          0.3559928443649374,
          0.36666666666666664
        ],
        "32": [
          0.32685512367491165,
          0.3175438596491228,
          0.3985890652557319,
          0.4329159212880143,
          0.4263157894736842
        ]
      },
      "best_layer_indices_reps": [
        30,
        31,
        32,
        32,
        32
      ],
      "best_layer_accuracies_reps": [
        0.38869257950530034,
        0.3736842105263158,
        0.3985890652557319,
        0.4329159212880143,
        0.4263157894736842
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.61      0.67      0.63        30\n           1       0.75      0.10      0.18        29\n           2       0.57      0.11      0.18        38\n           3       0.34      0.46      0.39        46\n           4       0.31      0.68      0.43        63\n           5       0.26      0.39      0.31        62\n           6       0.43      0.21      0.28        72\n           7       0.00      0.00      0.00        72\n           8       0.58      0.36      0.45        77\n           9       0.42      0.81      0.55        77\n\n    accuracy                           0.39       566\n   macro avg       0.43      0.38      0.34       566\nweighted avg       0.39      0.39      0.34       566\n",
        "              precision    recall  f1-score   support\n\n           0       0.50      0.44      0.47        25\n           1       0.19      0.18      0.19        28\n           2       0.33      0.25      0.29        40\n           3       0.33      0.52      0.40        50\n           4       0.44      0.44      0.44        64\n           5       0.45      0.08      0.13        64\n           6       0.18      0.06      0.09        71\n           7       0.36      0.56      0.44        73\n           8       0.33      0.65      0.44        78\n           9       0.65      0.42      0.51        77\n\n    accuracy                           0.37       570\n   macro avg       0.38      0.36      0.34       570\nweighted avg       0.39      0.37      0.35       570\n",
        "              precision    recall  f1-score   support\n\n           0       0.50      0.52      0.51        25\n           1       0.60      0.41      0.49        29\n           2       0.41      0.61      0.49        44\n           3       1.00      0.02      0.04        46\n           4       0.00      0.00      0.00        63\n           5       0.28      0.89      0.43        63\n           6       0.38      0.50      0.43        70\n           7       0.36      0.18      0.24        72\n           8       0.56      0.23      0.33        78\n           9       0.54      0.66      0.60        77\n\n    accuracy                           0.40       567\n   macro avg       0.46      0.40      0.36       567\nweighted avg       0.44      0.40      0.35       567\n",
        "              precision    recall  f1-score   support\n\n           0       0.49      0.89      0.63        27\n           1       0.69      0.36      0.47        25\n           2       1.00      0.05      0.09        42\n           3       0.50      0.31      0.38        42\n           4       0.32      0.42      0.36        60\n           5       0.50      0.12      0.20        64\n           6       0.28      0.80      0.42        71\n           7       0.41      0.32      0.36        71\n           8       0.71      0.34      0.46        79\n           9       0.66      0.69      0.68        78\n\n    accuracy                           0.43       559\n   macro avg       0.56      0.43      0.41       559\nweighted avg       0.54      0.43      0.41       559\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.11      0.19        28\n           1       0.54      0.54      0.54        26\n           2       0.28      0.57      0.37        40\n           3       0.68      0.36      0.47        47\n           4       0.11      0.02      0.03        62\n           5       0.50      0.14      0.22        65\n           6       0.37      0.52      0.43        73\n           7       0.31      0.73      0.43        73\n           8       0.84      0.33      0.47        79\n           9       0.59      0.77      0.67        77\n\n    accuracy                           0.43       570\n   macro avg       0.52      0.41      0.38       570\nweighted avg       0.50      0.43      0.39       570\n"
      ]
    },
    "800-809": {
      "target_sums": [
        800,
        801,
        802,
        803,
        804,
        805,
        806,
        807,
        808,
        809
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.70325,
        0.688,
        0.698,
        0.689,
        0.691
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2813,
        2752,
        2792,
        2756,
        2764
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.14031971580817051,
          0.14156079854809436,
          0.1449016100178891,
          0.14130434782608695,
          0.1410488245931284
        ],
        "2": [
          0.13676731793960922,
          0.14156079854809436,
          0.1413237924865832,
          0.13949275362318841,
          0.1410488245931284
        ],
        "3": [
          0.14031971580817051,
          0.15970961887477314,
          0.13774597495527727,
          0.1539855072463768,
          0.1410488245931284
        ],
        "4": [
          0.13854351687388988,
          0.1397459165154265,
          0.1449016100178891,
          0.16847826086956522,
          0.15913200723327306
        ],
        "5": [
          0.14209591474245115,
          0.16515426497277677,
          0.1449016100178891,
          0.1503623188405797,
          0.13381555153707053
        ],
        "6": [
          0.15452930728241562,
          0.16515426497277677,
          0.1449016100178891,
          0.16847826086956522,
          0.15370705244122965
        ],
        "7": [
          0.13143872113676733,
          0.18874773139745918,
          0.1967799642218247,
          0.15217391304347827,
          0.15370705244122965
        ],
        "8": [
          0.17406749555950266,
          0.20871143375680581,
          0.1806797853309481,
          0.19746376811594202,
          0.18806509945750452
        ],
        "9": [
          0.18294849023090587,
          0.21052631578947367,
          0.1967799642218247,
          0.19927536231884058,
          0.16636528028933092
        ],
        "10": [
          0.17939609236234458,
          0.16333938294010888,
          0.1735241502683363,
          0.213768115942029,
          0.15732368896925858
        ],
        "11": [
          0.15808170515097691,
          0.2014519056261343,
          0.19320214669051877,
          0.1721014492753623,
          0.14647377938517178
        ],
        "12": [
          0.19182948490230906,
          0.17967332123411978,
          0.22182468694096602,
          0.18297101449275363,
          0.20795660036166366
        ],
        "13": [
          0.18650088809946713,
          0.1996370235934664,
          0.21645796064400716,
          0.2210144927536232,
          0.189873417721519
        ],
        "14": [
          0.1847246891651865,
          0.20326678765880218,
          0.19320214669051877,
          0.21014492753623187,
          0.20976491862567812
        ],
        "15": [
          0.20426287744227353,
          0.2540834845735027,
          0.2182468694096601,
          0.2463768115942029,
          0.19529837251356238
        ],
        "16": [
          0.23090586145648312,
          0.2323049001814882,
          0.22361359570661896,
          0.22826086956521738,
          0.23688969258589512
        ],
        "17": [
          0.2522202486678508,
          0.2413793103448276,
          0.22719141323792486,
          0.2463768115942029,
          0.22603978300180833
        ],
        "18": [
          0.23978685612788633,
          0.22141560798548093,
          0.27191413237924866,
          0.2554347826086957,
          0.23869801084990958
        ],
        "19": [
          0.22735346358792186,
          0.23774954627949182,
          0.2737030411449016,
          0.2554347826086957,
          0.23508137432188064
        ],
        "20": [
          0.25754884547069273,
          0.22141560798548093,
          0.24865831842576028,
          0.24456521739130435,
          0.21338155515370705
        ],
        "21": [
          0.2468916518650089,
          0.2558983666061706,
          0.25760286225402507,
          0.20833333333333334,
          0.25316455696202533
        ],
        "22": [
          0.2699822380106572,
          0.26497277676951,
          0.27906976744186046,
          0.3061594202898551,
          0.27486437613019893
        ],
        "23": [
          0.2966252220248668,
          0.2885662431941924,
          0.25402504472271914,
          0.30978260869565216,
          0.298372513562387
        ],
        "24": [
          0.2699822380106572,
          0.294010889292196,
          0.33989266547406083,
          0.3170289855072464,
          0.24050632911392406
        ],
        "25": [
          0.29307282415630553,
          0.30490018148820325,
          0.27906976744186046,
          0.29528985507246375,
          0.30198915009041594
        ],
        "26": [
          0.3108348134991119,
          0.3157894736842105,
          0.32737030411449014,
          0.3496376811594203,
          0.3092224231464738
        ],
        "27": [
          0.3268206039076377,
          0.3484573502722323,
          0.3434704830053667,
          0.375,
          0.34900542495479203
        ],
        "28": [
          0.3285968028419183,
          0.35753176043557167,
          0.334525939177102,
          0.3894927536231884,
          0.3291139240506329
        ],
        "29": [
          0.3481349911190053,
          0.3811252268602541,
          0.3005366726296959,
          0.3605072463768116,
          0.37251356238698013
        ],
        "30": [
          0.38188277087033745,
          0.3629764065335753,
          0.3542039355992844,
          0.35507246376811596,
          0.36347197106690776
        ],
        "31": [
          0.369449378330373,
          0.32849364791288566,
          0.3363148479427549,
          0.3804347826086957,
          0.3471971066907776
        ],
        "32": [
          0.4422735346358792,
          0.3502722323049002,
          0.4007155635062612,
          0.42028985507246375,
          0.3743218806509946
        ]
      },
      "best_layer_indices_reps": [
        32,
        29,
        32,
        32,
        32
      ],
      "best_layer_accuracies_reps": [
        0.4422735346358792,
        0.3811252268602541,
        0.4007155635062612,
        0.42028985507246375,
        0.3743218806509946
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.88      0.62      0.73        24\n           1       0.89      0.25      0.39        32\n           2       0.65      0.33      0.44        39\n           3       0.33      0.80      0.46        49\n           4       0.44      0.11      0.18        62\n           5       0.37      0.62      0.46        61\n           6       0.00      0.00      0.00        69\n           7       0.55      0.22      0.31        73\n           8       0.38      0.94      0.54        79\n           9       0.72      0.52      0.60        75\n\n    accuracy                           0.44       563\n   macro avg       0.52      0.44      0.41       563\nweighted avg       0.47      0.44      0.39       563\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.48      0.65        25\n           1       0.44      0.68      0.54        28\n           2       0.69      0.24      0.36        37\n           3       0.25      0.43      0.32        46\n           4       0.31      0.08      0.13        59\n           5       0.34      0.20      0.25        60\n           6       0.37      0.33      0.35        69\n           7       0.28      0.31      0.29        74\n           8       0.47      0.36      0.41        78\n           9       0.40      0.79      0.53        75\n\n    accuracy                           0.38       551\n   macro avg       0.46      0.39      0.38       551\nweighted avg       0.41      0.38      0.36       551\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.30      0.47        23\n           1       0.58      0.58      0.58        26\n           2       0.33      0.77      0.46        39\n           3       0.46      0.36      0.40        45\n           4       0.38      0.15      0.21        61\n           5       0.47      0.44      0.46        63\n           6       0.36      0.53      0.43        73\n           7       1.00      0.01      0.03        74\n           8       0.50      0.05      0.09        78\n           9       0.38      0.97      0.55        77\n\n    accuracy                           0.40       559\n   macro avg       0.54      0.42      0.37       559\nweighted avg       0.52      0.40      0.33       559\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.33      0.50        24\n           1       0.30      0.83      0.44        23\n           2       0.49      0.57      0.53        37\n           3       0.38      0.33      0.36        45\n           4       0.00      0.00      0.00        62\n           5       0.55      0.38      0.45        63\n           6       0.31      0.81      0.44        72\n           7       0.39      0.12      0.19        73\n           8       0.77      0.13      0.22        78\n           9       0.53      0.91      0.67        75\n\n    accuracy                           0.42       552\n   macro avg       0.47      0.44      0.38       552\nweighted avg       0.45      0.42      0.36       552\n",
        "              precision    recall  f1-score   support\n\n           0       1.00      0.17      0.29        24\n           1       0.47      0.58      0.52        26\n           2       0.00      0.00      0.00        39\n           3       0.35      0.61      0.45        44\n           4       0.29      0.39      0.34        59\n           5       0.33      0.47      0.39        62\n           6       0.34      0.50      0.40        72\n           7       0.34      0.59      0.43        74\n           8       0.73      0.24      0.37        78\n           9       0.83      0.13      0.23        75\n\n    accuracy                           0.37       553\n   macro avg       0.47      0.37      0.34       553\nweighted avg       0.47      0.37      0.34       553\n"
      ]
    },
    "900-909": {
      "target_sums": [
        900,
        901,
        902,
        903,
        904,
        905,
        906,
        907,
        908,
        909
      ],
      "n_classes": 10,
      "generation_accuracies_reps": [
        0.63175,
        0.61325,
        0.6095,
        0.61975,
        0.61
      ],
      "original_sample_counts_reps": [
        4000,
        4000,
        4000,
        4000,
        4000
      ],
      "filtered_sample_counts_reps": [
        2527,
        2453,
        2438,
        2479,
        2440
      ],
      "probe_accuracies_per_layer_reps": {
        "1": [
          0.15217391304347827,
          0.15682281059063136,
          0.1598360655737705,
          0.15524193548387097,
          0.1557377049180328
        ],
        "2": [
          0.15810276679841898,
          0.164969450101833,
          0.16598360655737704,
          0.1592741935483871,
          0.16393442622950818
        ],
        "3": [
          0.14624505928853754,
          0.15682281059063136,
          0.16188524590163936,
          0.1532258064516129,
          0.15368852459016394
        ],
        "4": [
          0.15217391304347827,
          0.15682281059063136,
          0.1557377049180328,
          0.16733870967741934,
          0.1557377049180328
        ],
        "5": [
          0.16205533596837945,
          0.15682281059063136,
          0.1557377049180328,
          0.16330645161290322,
          0.18032786885245902
        ],
        "6": [
          0.1422924901185771,
          0.18126272912423624,
          0.18442622950819673,
          0.19153225806451613,
          0.1557377049180328
        ],
        "7": [
          0.17984189723320157,
          0.17922606924643583,
          0.1557377049180328,
          0.1693548387096774,
          0.16188524590163936
        ],
        "8": [
          0.2075098814229249,
          0.20570264765784113,
          0.21311475409836064,
          0.17137096774193547,
          0.21721311475409835
        ],
        "9": [
          0.23122529644268774,
          0.23014256619144602,
          0.20491803278688525,
          0.17540322580645162,
          0.20491803278688525
        ],
        "10": [
          0.21146245059288538,
          0.18737270875763748,
          0.22950819672131148,
          0.20766129032258066,
          0.2192622950819672
        ],
        "11": [
          0.1956521739130435,
          0.17515274949083504,
          0.22540983606557377,
          0.21774193548387097,
          0.19672131147540983
        ],
        "12": [
          0.2549407114624506,
          0.22606924643584522,
          0.19672131147540983,
          0.2318548387096774,
          0.22540983606557377
        ],
        "13": [
          0.2075098814229249,
          0.20977596741344195,
          0.23975409836065573,
          0.21975806451612903,
          0.1987704918032787
        ],
        "14": [
          0.2134387351778656,
          0.22810590631364563,
          0.22131147540983606,
          0.24193548387096775,
          0.2192622950819672
        ],
        "15": [
          0.24110671936758893,
          0.22606924643584522,
          0.23565573770491804,
          0.2923387096774194,
          0.26639344262295084
        ],
        "16": [
          0.274703557312253,
          0.2545824847250509,
          0.2848360655737705,
          0.26814516129032256,
          0.2725409836065574
        ],
        "17": [
          0.2727272727272727,
          0.26883910386965376,
          0.305327868852459,
          0.3064516129032258,
          0.2889344262295082
        ],
        "18": [
          0.25889328063241107,
          0.2505091649694501,
          0.27049180327868855,
          0.31048387096774194,
          0.2581967213114754
        ],
        "19": [
          0.25889328063241107,
          0.25865580448065173,
          0.26434426229508196,
          0.2903225806451613,
          0.2336065573770492
        ],
        "20": [
          0.25889328063241107,
          0.22606924643584522,
          0.26434426229508196,
          0.2842741935483871,
          0.26229508196721313
        ],
        "21": [
          0.2549407114624506,
          0.2729124236252546,
          0.2540983606557377,
          0.2903225806451613,
          0.27049180327868855
        ],
        "22": [
          0.2727272727272727,
          0.285132382892057,
          0.2725409836065574,
          0.32661290322580644,
          0.2930327868852459
        ],
        "23": [
          0.30237154150197626,
          0.2892057026476578,
          0.3094262295081967,
          0.3185483870967742,
          0.26434426229508196
        ],
        "24": [
          0.30632411067193677,
          0.26883910386965376,
          0.27049180327868855,
          0.35080645161290325,
          0.26229508196721313
        ],
        "25": [
          0.2845849802371542,
          0.34012219959266804,
          0.2807377049180328,
          0.3528225806451613,
          0.28688524590163933
        ],
        "26": [
          0.2984189723320158,
          0.3034623217922607,
          0.3073770491803279,
          0.3487903225806452,
          0.3237704918032787
        ],
        "27": [
          0.37549407114624506,
          0.3543788187372709,
          0.3155737704918033,
          0.3286290322580645,
          0.32991803278688525
        ],
        "28": [
          0.3675889328063241,
          0.32179226069246436,
          0.3237704918032787,
          0.33669354838709675,
          0.26844262295081966
        ],
        "29": [
          0.3458498023715415,
          0.34012219959266804,
          0.29098360655737704,
          0.3225806451612903,
          0.3401639344262295
        ],
        "30": [
          0.3458498023715415,
          0.38085539714867617,
          0.3360655737704918,
          0.3649193548387097,
          0.29918032786885246
        ],
        "31": [
          0.383399209486166,
          0.4093686354378819,
          0.375,
          0.33669354838709675,
          0.35450819672131145
        ],
        "32": [
          0.38537549407114624,
          0.3665987780040733,
          0.41188524590163933,
          0.3850806451612903,
          0.4262295081967213
        ]
      },
      "best_layer_indices_reps": [
        32,
        31,
        32,
        32,
        32
      ],
      "best_layer_accuracies_reps": [
        0.38537549407114624,
        0.4093686354378819,
        0.41188524590163933,
        0.3850806451612903,
        0.4262295081967213
      ],
      "best_layer_reports_reps": [
        "              precision    recall  f1-score   support\n\n           0       0.35      0.77      0.48        22\n           1       0.50      0.09      0.15        22\n           2       0.00      0.00      0.00        31\n           3       0.56      0.25      0.35        36\n           4       0.27      0.63      0.37        51\n           5       0.31      0.45      0.36        56\n           6       0.38      0.50      0.43        66\n           7       0.34      0.20      0.25        70\n           8       0.86      0.16      0.27        75\n           9       0.54      0.66      0.59        77\n\n    accuracy                           0.39       506\n   macro avg       0.41      0.37      0.33       506\nweighted avg       0.44      0.39      0.35       506\n",
        "              precision    recall  f1-score   support\n\n           0       0.48      0.63      0.55        19\n           1       0.50      0.33      0.40        21\n           2       0.22      0.07      0.11        29\n           3       0.26      0.27      0.27        37\n           4       0.53      0.17      0.26        47\n           5       0.40      0.31      0.35        54\n           6       0.33      0.56      0.41        63\n           7       0.38      0.28      0.32        69\n           8       0.51      0.41      0.46        75\n           9       0.46      0.78      0.58        77\n\n    accuracy                           0.41       491\n   macro avg       0.41      0.38      0.37       491\nweighted avg       0.41      0.41      0.39       491\n",
        "              precision    recall  f1-score   support\n\n           0       0.83      0.59      0.69        17\n           1       0.50      0.64      0.56        22\n           2       0.00      0.00      0.00        27\n           3       0.38      0.73      0.50        37\n           4       0.38      0.17      0.24        46\n           5       0.49      0.41      0.44        54\n           6       0.27      0.72      0.39        64\n           7       0.40      0.39      0.40        69\n           8       0.59      0.39      0.47        76\n           9       0.85      0.22      0.35        76\n\n    accuracy                           0.41       488\n   macro avg       0.47      0.43      0.40       488\nweighted avg       0.49      0.41      0.39       488\n",
        "              precision    recall  f1-score   support\n\n           0       0.43      1.00      0.60        19\n           1       1.00      0.05      0.09        22\n           2       0.83      0.16      0.26        32\n           3       0.47      0.41      0.43        37\n           4       0.57      0.28      0.38        46\n           5       0.25      0.62      0.36        52\n           6       0.67      0.15      0.24        67\n           7       0.33      0.40      0.36        68\n           8       0.40      0.86      0.55        76\n           9       1.00      0.05      0.10        77\n\n    accuracy                           0.39       496\n   macro avg       0.59      0.40      0.34       496\nweighted avg       0.58      0.39      0.33       496\n",
        "              precision    recall  f1-score   support\n\n           0       0.69      0.61      0.65        18\n           1       0.52      0.70      0.59        23\n           2       0.67      0.06      0.12        31\n           3       0.39      0.67      0.49        33\n           4       0.29      0.22      0.25        46\n           5       0.29      0.75      0.42        52\n           6       0.41      0.17      0.24        66\n           7       0.43      0.37      0.40        68\n           8       0.49      0.63      0.55        75\n           9       0.71      0.33      0.45        76\n\n    accuracy                           0.43       488\n   macro avg       0.49      0.45      0.42       488\nweighted avg       0.48      0.43      0.40       488\n"
      ]
    }
  },
  "best_layer_mode_per_range": {
    "500-509": 32,
    "600-609": 32,
    "700-709": 32,
    "800-809": 32,
    "900-909": 32
  }
}