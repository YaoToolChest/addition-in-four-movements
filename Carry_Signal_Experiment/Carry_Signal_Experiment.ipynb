{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "========== Running model: /root/autodl-tmp/AceMath ==========\n",
      "Outputs will be saved under: /root/EMNLP1 copy/Carry_Signal_Experiment/outputs/AceMath\n",
      "Loading tokenizer from /root/autodl-tmp/AceMath...\n",
      "Loading model from /root/autodl-tmp/AceMath with dtype=bf16 on cuda:0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21bc0cb37b54b1f9e10222a69b5a1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected hidden_states layers (including embeddings): 29\n",
      "Detected hidden_size: 3584\n",
      "Preparing datasets and dataloaders...\n",
      "  Pos 0: 2000 samples -> 4 batches\n",
      "  Pos 1: 2000 samples -> 4 batches\n",
      "  Pos 2: 2000 samples -> 4 batches\n",
      "CUDA Device Name: Tesla V100S-PCIE-32GB\n",
      "\n",
      "--- Repetition 1/5 with Seed 42 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 1 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 2/5 with Seed 43 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 2 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 3/5 with Seed 44 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 3 Positions: 100%|██████████| 3/3 [02:05<00:00, 41.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 4/5 with Seed 45 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 4 Positions: 100%|██████████| 3/3 [02:05<00:00, 41.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 5/5 with Seed 46 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 5 Positions: 100%|██████████| 3/3 [02:05<00:00, 41.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mean accuracies with CI to outputs/AceMath/carry_accuracies_mean_ci_nd3_nrep5.csv\n",
      "Saved plot to outputs/AceMath/carry_plot_mean_ci_nd3_nrep5.pdf\n",
      "\n",
      "========== Running model: /root/autodl-tmp/Qwen2.5-Math-7B ==========\n",
      "Outputs will be saved under: /root/EMNLP1 copy/Carry_Signal_Experiment/outputs/Qwen2.5-Math-7B\n",
      "Loading tokenizer from /root/autodl-tmp/Qwen2.5-Math-7B...\n",
      "Loading model from /root/autodl-tmp/Qwen2.5-Math-7B with dtype=bf16 on cuda:0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5433dcc0114e41aabe93f73ecce3e879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected hidden_states layers (including embeddings): 29\n",
      "Detected hidden_size: 3584\n",
      "Preparing datasets and dataloaders...\n",
      "  Pos 0: 2000 samples -> 4 batches\n",
      "  Pos 1: 2000 samples -> 4 batches\n",
      "  Pos 2: 2000 samples -> 4 batches\n",
      "CUDA Device Name: Tesla V100S-PCIE-32GB\n",
      "\n",
      "--- Repetition 1/5 with Seed 42 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 1 Positions: 100%|██████████| 3/3 [02:05<00:00, 41.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 2/5 with Seed 43 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 2 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 3/5 with Seed 44 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 3 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 4/5 with Seed 45 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 4 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 5/5 with Seed 46 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 5 Positions: 100%|██████████| 3/3 [02:05<00:00, 41.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mean accuracies with CI to outputs/Qwen2.5-Math-7B/carry_accuracies_mean_ci_nd3_nrep5.csv\n",
      "Saved plot to outputs/Qwen2.5-Math-7B/carry_plot_mean_ci_nd3_nrep5.pdf\n",
      "\n",
      "========== Running model: /root/autodl-tmp/Qwen2.5-7B-Instruct ==========\n",
      "Outputs will be saved under: /root/EMNLP1 copy/Carry_Signal_Experiment/outputs/Qwen2.5-7B-Instruct\n",
      "Loading tokenizer from /root/autodl-tmp/Qwen2.5-7B-Instruct...\n",
      "Loading model from /root/autodl-tmp/Qwen2.5-7B-Instruct with dtype=bf16 on cuda:0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906659606783402ea6547209b3911867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected hidden_states layers (including embeddings): 29\n",
      "Detected hidden_size: 3584\n",
      "Preparing datasets and dataloaders...\n",
      "  Pos 0: 2000 samples -> 4 batches\n",
      "  Pos 1: 2000 samples -> 4 batches\n",
      "  Pos 2: 2000 samples -> 4 batches\n",
      "CUDA Device Name: Tesla V100S-PCIE-32GB\n",
      "\n",
      "--- Repetition 1/5 with Seed 42 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 1 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 2/5 with Seed 43 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 2 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 3/5 with Seed 44 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 3 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 4/5 with Seed 45 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 4 Positions: 100%|██████████| 3/3 [02:07<00:00, 42.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repetition 5/5 with Seed 46 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rep 5 Positions: 100%|██████████| 3/3 [02:06<00:00, 42.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mean accuracies with CI to outputs/Qwen2.5-7B-Instruct/carry_accuracies_mean_ci_nd3_nrep5.csv\n",
      "Saved plot to outputs/Qwen2.5-7B-Instruct/carry_plot_mean_ci_nd3_nrep5.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math  # for sqrt when computing SEM\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\"\"\"\n",
    "model :\n",
    "NousResearch/Meta-Llama-3-8B-Instruct\n",
    "nvidia/AceMath-7B-Instruct\n",
    "mistralai/Mistral-7B-Instruct-v0.3\n",
    "Qwen/Qwen2.5-Math-7B-Instruct\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Config (Hyperparameters & Multi-Model Support)\n",
    "# ==============================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Support running multiple models at once; each item can be an HF Hub name\n",
    "    # (e.g., meta-llama/Meta-Llama-3-8B-Instruct) or a local path\n",
    "    # (e.g., /data/global/model/llama3_instruct/)\n",
    "    MODEL_NAMES: List[str] = field(default_factory=lambda: [\n",
    "        \"/root/autodl-tmp/llama\",\n",
    "        \"/root/autodl-tmp/Mistral\",\n",
    "        \"/root/autodl-tmp/AceMath\",\n",
    "        \"/root/autodl-tmp/Qwen2.5-Math-7B\",\n",
    "        \"/root/autodl-tmp/Qwen2.5-7B-Instruct\"\n",
    "    ])\n",
    "\n",
    "    # Some community weights require trust_remote_code\n",
    "    TRUST_REMOTE_CODE: bool = False\n",
    "\n",
    "    # Model precision auto selection: auto/bf16/fp16/fp32\n",
    "    DTYPE: str = \"auto\"\n",
    "\n",
    "    # Task/data-related\n",
    "    N_DIGITS: int = 3\n",
    "    POSITIONS: Tuple[int, ...] = (0, 1, 2)  # positions to probe (0 = least significant)\n",
    "    TASK_TO_PROBE: str = \"carry\"  # \"carry\" or \"digits\"\n",
    "    SAMPLES_PER_CLASS: int = 1000  # per class for \"carry\"\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE: int = 500\n",
    "    N_REPETITIONS: int = 5\n",
    "    BASE_SEED: int = 42\n",
    "    EPOCHS: int = 20\n",
    "    LR: float = 1e-3\n",
    "\n",
    "    # Device\n",
    "    DEVICE: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Output root directory; each model will create its own subfolder\n",
    "    # (using a sanitized version of the model name)\n",
    "    OUTPUT_ROOT: Path = Path(\"outputs\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Utilities\n",
    "# ==============================\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility across Python, NumPy, and PyTorch.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Optional: make behavior deterministic (may slow down)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def sanitize_model_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert an HF model name or local path into a safe folder name.\n",
    "    - Take the last path component if there's a '/'\n",
    "    - Replace non-alphanumeric characters with underscores\n",
    "    \"\"\"\n",
    "    # Take the \"last component\"\n",
    "    last = name.rstrip(\"/\").split(\"/\")[-1] if \"/\" in name.rstrip(\"/\") else name\n",
    "    # If it's a disk path but the last component is empty, fall back one level\n",
    "    if last == \"\":\n",
    "        last = Path(name).name\n",
    "    # Replace illegal characters\n",
    "    safe = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", last)\n",
    "    return safe if safe else \"model\"\n",
    "\n",
    "\n",
    "def pick_dtype(device: str, pref: str = \"auto\"):\n",
    "    \"\"\"\n",
    "    Choose torch dtype based on device and preference.\n",
    "    Returns (torch.dtype, textual_description)\n",
    "    \"\"\"\n",
    "    if pref.lower() == \"bf16\":\n",
    "        return torch.bfloat16, \"bf16\"\n",
    "    if pref.lower() == \"fp16\":\n",
    "        return torch.float16, \"fp16\"\n",
    "    if pref.lower() == \"fp32\":\n",
    "        return torch.float32, \"fp32\"\n",
    "\n",
    "    # auto\n",
    "    if \"cuda\" in device:\n",
    "        # Prefer bf16 (Ampere and newer), otherwise fallback to fp16\n",
    "        # We won't probe hardware here; simply try bf16 -> fallback to fp16\n",
    "        try:\n",
    "            return torch.bfloat16, \"bf16\"\n",
    "        except Exception:\n",
    "            return torch.float16, \"fp16\"\n",
    "    else:\n",
    "        return torch.float32, \"fp32\"\n",
    "\n",
    "\n",
    "def generate_addition_sample(n_digits: int) -> Tuple[str, List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate one a+b sample, returning:\n",
    "    - prompt string,\n",
    "    - carry flags list (length n_digits+1, LSB first),\n",
    "    - sum digits list (length n_digits+1, LSB first).\n",
    "    \"\"\"\n",
    "    lo = 10 ** (n_digits - 1) if n_digits > 0 else 0\n",
    "    hi = (10 ** n_digits) - 1 if n_digits > 0 else 0\n",
    "    a = random.randint(lo, hi)\n",
    "    b = random.randint(lo, hi)\n",
    "    c_val = a + b\n",
    "    prompt = f\"Calculate: {a}+{b} = \"\n",
    "\n",
    "    num_positions = n_digits + 1\n",
    "\n",
    "    def to_digits(x: int, length: int) -> List[int]:\n",
    "        s = f\"{x:0{length}d}\"[::-1]  # reverse string so LSB is first\n",
    "        return [int(d) for d in s]\n",
    "\n",
    "    A = to_digits(a, num_positions)\n",
    "    B = to_digits(b, num_positions)\n",
    "    C = to_digits(c_val, num_positions)\n",
    "\n",
    "    carry_flags = []\n",
    "    carry = 0\n",
    "    for i in range(num_positions):\n",
    "        s = A[i] + B[i] + carry\n",
    "        if s >= 10:\n",
    "            carry_flags.append(1)\n",
    "            carry = 1\n",
    "        else:\n",
    "            carry_flags.append(0)\n",
    "            carry = 0\n",
    "    return prompt, carry_flags, C\n",
    "\n",
    "\n",
    "class BalancedAdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For task=\"carry\": balance classes (carry=1/0) at a target position.\n",
    "    For task=\"digits\": do not enforce 10-class balance; just collect total samples.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_digits: int,\n",
    "        target_pos: int,\n",
    "        samples_per_class: int,\n",
    "        tokenizer,\n",
    "        task: str = \"carry\",\n",
    "    ):\n",
    "        yes, no = [], []\n",
    "        max_attempts = samples_per_class * 100  # avoid infinite loops\n",
    "        attempts = 0\n",
    "\n",
    "        while (len(yes) < samples_per_class or len(no) < samples_per_class) and attempts < max_attempts:\n",
    "            prompt, carry_flags, digits = generate_addition_sample(n_digits)\n",
    "\n",
    "            if target_pos >= len(carry_flags):\n",
    "                attempts += 1\n",
    "                continue\n",
    "\n",
    "            if task == \"carry\":\n",
    "                flag_val = carry_flags[target_pos]\n",
    "                if flag_val == 1 and len(yes) < samples_per_class:\n",
    "                    yes.append((prompt, carry_flags, digits))\n",
    "                elif flag_val == 0 and len(no) < samples_per_class:\n",
    "                    no.append((prompt, carry_flags, digits))\n",
    "            elif task == \"digits\":\n",
    "                if len(yes) < samples_per_class * 2:\n",
    "                    yes.append((prompt, carry_flags, digits))\n",
    "                else:\n",
    "                    if len(no) < samples_per_class:\n",
    "                        no = [None] * samples_per_class  # sentinel to exit\n",
    "            attempts += 1\n",
    "\n",
    "        if task == \"carry\" and (len(yes) < samples_per_class or len(no) < samples_per_class):\n",
    "            print(\n",
    "                f\"Warning: Could not generate enough balanced samples for 'carry' at position {target_pos}. \"\n",
    "                f\"Yes: {len(yes)}, No: {len(no)}\"\n",
    "            )\n",
    "        elif task == \"digits\" and len(yes) < samples_per_class * 2:\n",
    "            print(\n",
    "                f\"Warning: Could not generate enough samples for 'digits' at position {target_pos}. \"\n",
    "                f\"Collected: {len(yes)}, Expected: {samples_per_class * 2}\"\n",
    "            )\n",
    "\n",
    "        self.samples = yes + (no if task == \"carry\" else [])\n",
    "        if task == \"digits\":\n",
    "            self.samples = [s for s in self.samples if s is not None]  # drop sentinel items\n",
    "\n",
    "        self.task = task\n",
    "        self.n_digits_plus_1 = n_digits + 1\n",
    "        self.tokenizer = tokenizer\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt, carry_flags, digits = self.samples[idx]\n",
    "        expected_len = self.n_digits_plus_1\n",
    "\n",
    "        # Ensure consistent length\n",
    "        if len(carry_flags) < expected_len:\n",
    "            carry_flags.extend([0] * (expected_len - len(carry_flags)))\n",
    "        if len(digits) < expected_len:\n",
    "            digits.extend([0] * (expected_len - len(digits)))\n",
    "\n",
    "        enc = self.tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=True)\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids[0],\n",
    "            \"attention_mask\": enc.attention_mask[0],\n",
    "            \"carry_flags\": torch.tensor(carry_flags, dtype=torch.long),\n",
    "            \"digits\": torch.tensor(digits, dtype=torch.long),\n",
    "            \"prompt_text\": prompt,\n",
    "        }\n",
    "\n",
    "\n",
    "def collate(batch: List[Dict], pad_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"Pad tokenized inputs; stack labels.\"\"\"\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    coll: Dict[str, Any] = {}\n",
    "    keys = batch[0].keys()\n",
    "    for key in keys:\n",
    "        if key in (\"input_ids\", \"attention_mask\"):\n",
    "            coll[key] = pad_sequence([b[key] for b in batch], batch_first=True, padding_value=pad_id)\n",
    "        elif key == \"prompt_text\":\n",
    "            coll[key] = [b[key] for b in batch]\n",
    "        else:  # carry_flags, digits\n",
    "            coll[key] = torch.stack([b[key] for b in batch])\n",
    "    return coll\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, in_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class MLPProbe(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer MLP:\n",
    "        in_dim → hidden_dim (ReLU) → n_classes\n",
    "    Default hidden_dim = in_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, n_classes: int, hidden_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or in_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_classes, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "def save_probe_model(probe, layer_idx, pos_val, rep_idx, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Save probe weights to:\n",
    "      {out_dir}/probes/pos{pos}/probe_layer{layer}_rep{rep}.pt\n",
    "    \"\"\"\n",
    "    save_dir = out_dir / \"probes\" / f\"pos{pos_val}\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = save_dir / f\"probe_layer{layer_idx}_rep{rep_idx}.pt\"\n",
    "    torch.save(probe.state_dict(), model_save_path)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Helper: infer hidden-states layer count and dimension with a single minimal forward pass\n",
    "# ==============================\n",
    "@torch.no_grad()\n",
    "def infer_hs_layers_and_dim(model, tokenizer, device: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Return (number_of_hidden_states_layers L, hidden_size).\n",
    "    Note: L typically equals 1 (embedding) + num_hidden_layers.\n",
    "    \"\"\"\n",
    "    tok = tokenizer(\"0+0=\", return_tensors=\"pt\", padding=False, truncation=True)\n",
    "    tok = {k: v.to(device) for k, v in tok.items()}\n",
    "    outs = model(**tok, output_hidden_states=True)\n",
    "    hs = outs.hidden_states  # tuple/list of [B, T, H]\n",
    "    L = len(hs)\n",
    "    H = hs[-1].shape[-1]\n",
    "    return L, H\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Training: one forward per batch; train probes for all layers\n",
    "# ==============================\n",
    "\n",
    "@torch.no_grad()\n",
    "def cache_features_for_loader(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device: str,\n",
    "    store_dtype: torch.dtype = torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dataloader 里的所有样本只做一次 LLM 前向，缓存：\n",
    "      feats: [L, N, H]（所有层的“最后 token”向量，CPU/fp16）\n",
    "      carry: [N, P]    （carry_flags）\n",
    "      digits:[N, P]    （sum digits）\n",
    "    返回 (feats_LNH, carry, digits)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 关闭 KV cache，纯取 hidden_states 更快更省显存\n",
    "    if hasattr(model, \"config\") and getattr(model.config, \"use_cache\", None) is not None:\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    feats_list = []\n",
    "    carry_list = []\n",
    "    digits_list = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attn = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "            outs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attn,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            hs = outs.hidden_states  # list of [B, T, H]\n",
    "            idxs = attn.sum(1) - 1  # 每个样本的最后有效 token 下标（[B]）\n",
    "\n",
    "            # [L, B, H]：拼每层的“最后 token”向量\n",
    "            last_per_layer = torch.stack(\n",
    "                [h[torch.arange(h.size(0), device=h.device), idxs] for h in hs],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "            feats_list.append(last_per_layer.to(dtype=store_dtype).cpu())\n",
    "            carry_list.append(batch[\"carry_flags\"].cpu())\n",
    "            digits_list.append(batch[\"digits\"].cpu())\n",
    "\n",
    "    feats = torch.cat(feats_list, dim=1)      # [L, N, H]\n",
    "    carry = torch.cat(carry_list, dim=0)      # [N, P]\n",
    "    digits = torch.cat(digits_list, dim=0)    # [N, P]\n",
    "    return feats, carry, digits\n",
    "\n",
    "def train_all_layers_for_one_position(\n",
    "    model,                 # 已不再用于训练，只用于 cache，保留参数以最小化改动\n",
    "    probes_per_layer: Dict[int, nn.Module],  # {layer_idx: probe}\n",
    "    dataloader,           # 用它来做一次特征缓存\n",
    "    position: int,\n",
    "    device: str,\n",
    "    task: str,            # \"carry\" or \"digits\"\n",
    "    rep_idx: int,\n",
    "    out_dir: Path,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    一次前向缓存特征 -> 多个 epoch 循环只在缓存上训练/评估探针。\n",
    "    \"\"\"\n",
    "    # 1) 只做一次 LLM 前向，缓存 [L, N, H] 特征 + 标签\n",
    "    feats_LNH, labels_carry, labels_digits = cache_features_for_loader(\n",
    "        model, dataloader, device, store_dtype=torch.float16\n",
    "    )\n",
    "    # 让探针训练时用 fp32；特征在搬到设备时再转 fp32\n",
    "    probe_dtype = torch.float32\n",
    "\n",
    "    # 2) 生成当前 position 的标签向量 y: [N]\n",
    "    if position >= labels_carry.size(1):  # 安全防守\n",
    "        return {l: 0.0 for l in probes_per_layer}\n",
    "\n",
    "    y_all = labels_carry[:, position] if task == \"carry\" else labels_digits[:, position]\n",
    "    y_all = y_all.to(torch.long)\n",
    "    N = y_all.numel()\n",
    "\n",
    "    # 3) 初始化优化器/损失\n",
    "    for p in probes_per_layer.values():\n",
    "        p.train()\n",
    "        p.to(device=device, dtype=probe_dtype)\n",
    "    opts = {l: torch.optim.AdamW(p.parameters(), lr=lr) for l, p in probes_per_layer.items()}\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 为了复用批大小，取自 dataloader（若取不到就回退 512）\n",
    "    try:\n",
    "        batch_size = dataloader.batch_size or 512\n",
    "    except Exception:\n",
    "        batch_size = 512\n",
    "\n",
    "    # 4) 训练（仅在缓存特征上进行）\n",
    "    for _ in range(epochs):\n",
    "        perm = torch.randperm(N)\n",
    "        for s in range(0, N, batch_size):\n",
    "            idx = perm[s : s + batch_size]\n",
    "            yb = y_all[idx].to(device)\n",
    "\n",
    "            # 针对每一层的探针，取该层对应的特征切片 [b, H]\n",
    "            for l, probe in probes_per_layer.items():\n",
    "                xb = feats_LNH[l, idx].to(device=device, dtype=probe_dtype)  # fp16->fp32\n",
    "                logits = probe(xb)\n",
    "                loss = crit(logits.float(), yb)\n",
    "\n",
    "                opts[l].zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                opts[l].step()\n",
    "\n",
    "    # 5) 评估（同样只用缓存特征）\n",
    "    for p in probes_per_layer.values():\n",
    "        p.eval()\n",
    "\n",
    "    correct = {l: 0 for l in probes_per_layer}\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, N, batch_size):\n",
    "            idx = slice(s, min(s + batch_size, N))\n",
    "            yb_cpu = y_all[idx]  # CPU long\n",
    "            for l, probe in probes_per_layer.items():\n",
    "                xb = feats_LNH[l, idx].to(device=device, dtype=probe_dtype)\n",
    "                pred = probe(xb).argmax(-1).cpu()\n",
    "                correct[l] += (pred == yb_cpu).sum().item()\n",
    "\n",
    "    acc_by_layer = {l: (correct[l] / N if N else 0.0) for l in probes_per_layer}\n",
    "\n",
    "    # 6) 保存探针\n",
    "    for l, probe in probes_per_layer.items():\n",
    "        save_probe_model(probe, l, position, rep_idx, out_dir)\n",
    "\n",
    "    return acc_by_layer\n",
    "\n",
    "\n",
    "\n",
    "def run_one_model(cfg: Config, model_name: str):\n",
    "    \"\"\"Run the full pipeline for a single model: load → infer layers → train/eval → save summary.\"\"\"\n",
    "    outdir = cfg.OUTPUT_ROOT / sanitize_model_name(model_name)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n========== Running model: {model_name} ==========\")\n",
    "    print(f\"Outputs will be saved under: {outdir.resolve()}\")\n",
    "\n",
    "    # Choose dtype\n",
    "    model_dtype, dtype_name = pick_dtype(cfg.DEVICE, cfg.DTYPE)\n",
    "\n",
    "    # --- Load tokenizer & model ---\n",
    "    print(f\"Loading tokenizer from {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=cfg.TRUST_REMOTE_CODE)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Tokenizer pad_token was None; set to eos_token: {tokenizer.eos_token}\")\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(f\"Loading model from {model_name} with dtype={dtype_name} on {cfg.DEVICE} ...\")\n",
    "    if \"cuda\" not in cfg.DEVICE and dtype_name != \"fp32\":\n",
    "        print(\"Warning: CPU device detected; dtype forced to fp32 for stability.\")\n",
    "        model_dtype = torch.float32\n",
    "        dtype_name = \"fp32\"\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            output_hidden_states=True,\n",
    "            torch_dtype=model_dtype,\n",
    "            trust_remote_code=cfg.TRUST_REMOTE_CODE\n",
    "        ).to(cfg.DEVICE).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Please ensure MODEL_NAMES are valid and you have permissions if it is private.\")\n",
    "        print(\"If VRAM is limited, consider a smaller model (e.g., 'gpt2') for testing.\")\n",
    "        return\n",
    "\n",
    "    # --- Infer actual number of hidden-state layers via one minimal forward ---\n",
    "    n_layers_hs, hidden_size = infer_hs_layers_and_dim(model, tokenizer, cfg.DEVICE)\n",
    "    # Note: layer 0 is the embedding, and the last layer is the final layer\n",
    "    print(f\"Detected hidden_states layers (including embeddings): {n_layers_hs}\")\n",
    "    print(f\"Detected hidden_size: {hidden_size}\")\n",
    "\n",
    "    # --- Pre-build DataLoaders (one per position; balanced for 'carry') ---\n",
    "    loaders = {}\n",
    "    print(\"Preparing datasets and dataloaders...\")\n",
    "    for pos_val in cfg.POSITIONS:\n",
    "        ds = BalancedAdditionDataset(cfg.N_DIGITS, pos_val, cfg.SAMPLES_PER_CLASS, tokenizer, task=cfg.TASK_TO_PROBE)\n",
    "        loaders[pos_val] = DataLoader(\n",
    "            ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda b, pid=pad_id: collate(b, pid),\n",
    "        )\n",
    "        print(f\"  Pos {pos_val}: {len(ds)} samples -> {len(loaders[pos_val])} batches\")\n",
    "        expected_samples = cfg.SAMPLES_PER_CLASS * 2 if cfg.TASK_TO_PROBE == \"carry\" else cfg.SAMPLES_PER_CLASS * 2\n",
    "        if len(ds) < expected_samples:\n",
    "            print(\n",
    "                f\"  Warning: Dataset for Pos {pos_val} (Task: {cfg.TASK_TO_PROBE}) has fewer samples \"\n",
    "                f\"({len(ds)}) than expected ({expected_samples}).\"\n",
    "            )\n",
    "\n",
    "    # --- Multiple repetitions: store accuracies with shape (R, L, P) ---\n",
    "    all_runs_accuracies_list: List[List[List[float]]] = []\n",
    "\n",
    "    if \"cuda\" in cfg.DEVICE:\n",
    "        try:\n",
    "            print(f\"CUDA Device Name: {torch.cuda.get_device_name(torch.device(cfg.DEVICE))}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for rep_idx in range(cfg.N_REPETITIONS):\n",
    "        current_seed = cfg.BASE_SEED + rep_idx\n",
    "        set_seed(current_seed)\n",
    "        print(f\"\\n--- Repetition {rep_idx + 1}/{cfg.N_REPETITIONS} with Seed {current_seed} ---\")\n",
    "\n",
    "        # Rebuild probes each repetition\n",
    "        accuracies_current_run: List[List[float]] = [[0.0] * len(cfg.POSITIONS) for _ in range(n_layers_hs)]\n",
    "\n",
    "        # Iterate positions: one forward per batch to train all layer probes\n",
    "        for pos_list_idx, pos_val in enumerate(tqdm(cfg.POSITIONS, desc=f\"Rep {rep_idx + 1} Positions\")):\n",
    "            num_classes_probe = 2 if cfg.TASK_TO_PROBE == \"carry\" else 10\n",
    "            probes_for_all_layers: Dict[int, nn.Module] = {\n",
    "                l: LinearProbe(hidden_size, num_classes_probe) for l in range(n_layers_hs)\n",
    "            }\n",
    "            accs = train_all_layers_for_one_position(\n",
    "                model,\n",
    "                probes_for_all_layers,\n",
    "                loaders[pos_val],\n",
    "                position=pos_val,\n",
    "                device=cfg.DEVICE,\n",
    "                task=cfg.TASK_TO_PROBE,\n",
    "                rep_idx=rep_idx,\n",
    "                out_dir=outdir,\n",
    "                epochs=cfg.EPOCHS,\n",
    "                lr=cfg.LR,\n",
    "            )\n",
    "            for l in range(n_layers_hs):\n",
    "                accuracies_current_run[l][pos_list_idx] = accs[l]\n",
    "\n",
    "        all_runs_accuracies_list.append(accuracies_current_run)\n",
    "\n",
    "    # --- Aggregate statistics and save CSV ---\n",
    "    if not all_runs_accuracies_list:\n",
    "        print(\"No accuracy data collected. Skipping save for this model.\")\n",
    "        # Free VRAM\n",
    "        del model\n",
    "        if \"cuda\" in cfg.DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "        return\n",
    "\n",
    "    all_runs_accuracies_np = np.array(all_runs_accuracies_list)  # (R, L, P)\n",
    "    mean_accuracies = np.mean(all_runs_accuracies_np, axis=0)\n",
    "    std_dev_accuracies = np.std(all_runs_accuracies_np, axis=0)\n",
    "\n",
    "    # SEM & 95% CI\n",
    "    sem_accuracies = std_dev_accuracies / math.sqrt(cfg.N_REPETITIONS) if cfg.N_REPETITIONS > 0 else np.zeros_like(std_dev_accuracies)\n",
    "    z_score = 1.96\n",
    "    ci_lower_bounds = np.clip(mean_accuracies - z_score * sem_accuracies, 0, 1)\n",
    "    ci_upper_bounds = np.clip(mean_accuracies + z_score * sem_accuracies, 0, 1)\n",
    "\n",
    "    csv_path = outdir / f\"{cfg.TASK_TO_PROBE}_accuracies_mean_ci_nd{cfg.N_DIGITS}_nrep{cfg.N_REPETITIONS}.csv\"\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"layer\"] + [f\"pos_{p}_mean,pos_{p}_ci_lower,pos_{p}_ci_upper,pos_{p}_std_dev\" for p in cfg.POSITIONS]\n",
    "        # Flatten the comma-separated group in header\n",
    "        flat_header = [\"layer\"]\n",
    "        for p in cfg.POSITIONS:\n",
    "            flat_header.extend([f\"pos_{p}_mean\", f\"pos_{p}_ci_lower\", f\"pos_{p}_ci_upper\", f\"pos_{p}_std_dev\"])\n",
    "        w.writerow(flat_header)\n",
    "        for i in range(n_layers_hs):\n",
    "            row_data = [i]\n",
    "            for j in range(len(cfg.POSITIONS)):\n",
    "                row_data.extend([\n",
    "                    float(mean_accuracies[i, j]),\n",
    "                    float(ci_lower_bounds[i, j]),\n",
    "                    float(ci_upper_bounds[i, j]),\n",
    "                    float(std_dev_accuracies[i, j]),\n",
    "                ])\n",
    "            w.writerow(row_data)\n",
    "    print(f\"Saved mean accuracies with CI to {csv_path}\")\n",
    "\n",
    "    # --- Plot and save (kept consistent with the original) ---\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.rcParams.update({\n",
    "            'font.size': 14,\n",
    "            'font.family': 'serif',\n",
    "            'font.sans-serif': ['Arial'],\n",
    "            'axes.labelsize': 14,\n",
    "            'axes.titlesize': 20,\n",
    "            'xtick.labelsize': 12,\n",
    "            'ytick.labelsize': 12,\n",
    "            'legend.fontsize': 14,\n",
    "            'lines.linewidth': 1.8,\n",
    "            'lines.markersize': 6,\n",
    "            'axes.grid': True,\n",
    "            'grid.alpha': 0.5,\n",
    "            'grid.linestyle': ':',\n",
    "            'savefig.dpi': 600,\n",
    "            'savefig.format': 'pdf',\n",
    "            'savefig.bbox': 'tight',\n",
    "        })\n",
    "\n",
    "        position_descriptive_labels = {\n",
    "            0: \"ones place\",\n",
    "            1: \"tens place\",\n",
    "            2: \"hundreds place\",\n",
    "        }\n",
    "\n",
    "        colors = plt.cm.winter(np.linspace(0, 1, len(cfg.POSITIONS)))\n",
    "\n",
    "        for pos_list_idx, pos_val in enumerate(cfg.POSITIONS):\n",
    "            means = mean_accuracies[:, pos_list_idx]\n",
    "            ci_low = ci_lower_bounds[:, pos_list_idx]\n",
    "            ci_high = ci_upper_bounds[:, pos_list_idx]\n",
    "\n",
    "            label_text = position_descriptive_labels.get(pos_val, f\"Pos {pos_val}\")\n",
    "            plot_legend_label = f\"{label_text} (Mean Acc.)\"\n",
    "            # Leading underscore: do not display this item in legend\n",
    "            fill_legend_label = f\"_{label_text} (95% CI)\"\n",
    "\n",
    "            plt.plot(range(n_layers_hs), means, marker='o', linestyle='-', color=colors[pos_list_idx], label=plot_legend_label)\n",
    "            plt.fill_between(range(n_layers_hs), ci_low, ci_high, color=colors[pos_list_idx], alpha=0.2, label=fill_legend_label)\n",
    "\n",
    "        model_tag = sanitize_model_name(model_name)  \n",
    "        plt.xlabel(f\"Layer ({model_tag})\")\n",
    "        plt.ylabel(\"Mean Probe Accuracy\")\n",
    "        plt.title(f\"{cfg.TASK_TO_PROBE.capitalize()} Probe: Mean Accuracy by Layer & Position\\n\"\n",
    "                  f\"({cfg.N_REPETITIONS} Runs, N_digits: {cfg.N_DIGITS})\",\n",
    "                  fontsize=14)\n",
    "        plt.legend(loc='best', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.ylim(0.45, 1.05)\n",
    "        plt.xticks(range(n_layers_hs))\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path = outdir / f\"{cfg.TASK_TO_PROBE}_plot_mean_ci_nd{cfg.N_DIGITS}_nrep{cfg.N_REPETITIONS}.pdf\"\n",
    "        plt.savefig(plot_path, format='pdf', dpi=600)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot to {plot_path}\")\n",
    "    except ImportError:\n",
    "        print(\"matplotlib is not installed; skipping plot.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during plotting: {e}\")\n",
    "\n",
    "    # --- Record run metadata ---\n",
    "    meta = {\n",
    "        \"model_name\": model_name,\n",
    "        \"sanitized_dir\": sanitize_model_name(model_name),\n",
    "        \"device\": cfg.DEVICE,\n",
    "        \"dtype\": dtype_name,\n",
    "        \"n_digits\": cfg.N_DIGITS,\n",
    "        \"positions\": list(cfg.POSITIONS),\n",
    "        \"task\": cfg.TASK_TO_PROBE,\n",
    "        \"samples_per_class\": cfg.SAMPLES_PER_CLASS,\n",
    "        \"batch_size\": cfg.BATCH_SIZE,\n",
    "        \"repetitions\": cfg.N_REPETITIONS,\n",
    "        \"epochs\": cfg.EPOCHS,\n",
    "        \"lr\": cfg.LR,\n",
    "        \"detected_layers_hidden_states\": n_layers_hs,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"csv_path\": str(csv_path),\n",
    "    }\n",
    "    with open(outdir / \"run_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Free VRAM\n",
    "    del model\n",
    "    if \"cuda\" in cfg.DEVICE:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "\n",
    "    # Create root output directory\n",
    "    cfg.OUTPUT_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print(f\"Using device: {cfg.DEVICE}\")\n",
    "    if \"cuda\" not in cfg.DEVICE and cfg.DTYPE.lower() != \"fp32\":\n",
    "        print(\"Info: On CPU, dtype will be fp32 regardless of config.DTYPE.\")\n",
    "\n",
    "    # Run over models\n",
    "    for model_name in cfg.MODEL_NAMES:\n",
    "        run_one_model(cfg, model_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75caef29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
