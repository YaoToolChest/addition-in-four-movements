{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61977d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL 1/5: /root/autodl-tmp/llama =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbc9f80586e4b86ae17735b9bcaa4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3657/1106130021.py:666: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', 5)\n",
      "/tmp/ipykernel_3657/1106130021.py:738: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors_map = plt.cm.get_cmap('tab10', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL 2/5: /root/autodl-tmp/AceMath =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a149b4ac3b044e31963bbd59b7d2340f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3657/1106130021.py:666: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', 5)\n",
      "/tmp/ipykernel_3657/1106130021.py:738: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors_map = plt.cm.get_cmap('tab10', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL 3/5: /root/autodl-tmp/Mistral =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99af489ce27e44b4bf7aef4c614f81d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3657/1106130021.py:666: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', 5)\n",
      "/tmp/ipykernel_3657/1106130021.py:738: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors_map = plt.cm.get_cmap('tab10', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL 4/5: /root/autodl-tmp/Qwen2.5-7B-Instruct =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91105e8c25b2484dba643230e67818d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3657/1106130021.py:666: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', 5)\n",
      "/tmp/ipykernel_3657/1106130021.py:738: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors_map = plt.cm.get_cmap('tab10', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL 5/5: /root/autodl-tmp/Qwen2.5-Math-7B =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b109ec113a49729f915cea74556d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3657/1106130021.py:666: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', 5)\n",
      "/tmp/ipykernel_3657/1106130021.py:738: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors_map = plt.cm.get_cmap('tab10', 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> All models finished (see per-model logs under ./outputs/<model_name_short>/).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # non-interactive backend (no GUI)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ========== Global logging ==========\n",
    "logger = logging.getLogger(\"probe\")\n",
    "logger.propagate = False  # avoid duplicate output to root logger\n",
    "\n",
    "\n",
    "def setup_logger(log_file_path: str, also_console: bool = False, console_level=logging.WARNING):\n",
    "    \"\"\"Configure the logger: write to a file and optionally also to the console.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file to write.\n",
    "        also_console: If True, also log to the console.\n",
    "        console_level: Console log level if also_console is True.\n",
    "    \"\"\"\n",
    "    logger.setLevel(logging.INFO)\n",
    "    for h in list(logger.handlers):\n",
    "        logger.removeHandler(h)\n",
    "\n",
    "    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "    file_handler = logging.FileHandler(log_file_path, encoding=\"utf-8\")\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    fmt = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "    file_handler.setFormatter(fmt)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    if also_console:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(console_level)\n",
    "        console_handler.setFormatter(fmt)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    logger.info(\"======= Logger initialized =======\")\n",
    "    logger.info(f\"Log file: {log_file_path}\")\n",
    "\n",
    "\n",
    "# --- Global Configuration (each model runs the main pipeline separately) ---\n",
    "# ########################################################################\n",
    "#                          ↓↓↓ Global config ↓↓↓                         #\n",
    "# ########################################################################\n",
    "\n",
    "# Put multiple HF repo IDs or local paths here (ensure they are available)\n",
    "MODELS = [\n",
    "    \"/root/autodl-tmp/llama\",\n",
    "    \"/root/autodl-tmp/AceMath\",\n",
    "    \"/root/autodl-tmp/Mistral\",\n",
    "    \"/root/autodl-tmp/Qwen2.5-7B-Instruct\",\n",
    "    \"/root/autodl-tmp/Qwen2.5-Math-7B\"\n",
    "]\n",
    "\n",
    "def sanitize_model_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert an HF model name or local path into a safe folder name.\n",
    "    - Take the last path component if there's a '/'\n",
    "    - Replace non-alphanumeric characters with underscores\n",
    "    \"\"\"\n",
    "    # Take the \"last component\"\n",
    "    last = name.rstrip(\"/\").split(\"/\")[-1] if \"/\" in name.rstrip(\"/\") else name\n",
    "    # If it's a disk path but the last component is empty, fall back one level\n",
    "    if last == \"\":\n",
    "        last = Path(name).name\n",
    "    # Replace illegal characters\n",
    "    safe = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", last)\n",
    "    return safe if safe else \"model\"\n",
    "\n",
    "DEVICE_ID = 0\n",
    "\n",
    "# === Automatic layer selection flags ===\n",
    "AUTO_INCLUDE_EMBEDDING_STATE = False  # True includes hidden_states[0] (embeddings) for probing\n",
    "AUTO_EVERY_K_LAYERS = 1               # Probe every k layers (use 2/3/4 for large models)\n",
    "AUTO_LIMIT_MAX_LAYERS = None          # Limit to first N layers; None = no limit\n",
    "USE_TQDM = False                      # Disable progress bar (key milestones logged instead)\n",
    "\n",
    "# --- 3-Way Commutativity Probe Config ---\n",
    "N_SAMPLES_PER_CLASS_3WAY = 500\n",
    "TOTAL_SAMPLES_3WAY = N_SAMPLES_PER_CLASS_3WAY * 3\n",
    "BATCH_SIZE_PROBE = 64\n",
    "NUM_EPOCHS_PROBE = 10\n",
    "LEARNING_RATE_PROBE = 0.001\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "OUTPUT_DIM_3WAY = 3\n",
    "\n",
    "# Multiple replicates (each model will run with these seeds)\n",
    "N_REPLICATES = 5\n",
    "RANDOM_SEEDS_LIST = [42, 43, 44, 45, 46]\n",
    "if len(RANDOM_SEEDS_LIST) != N_REPLICATES:\n",
    "    raise ValueError(\"Length of RANDOM_SEEDS_LIST must be equal to N_REPLICATES\")\n",
    "\n",
    "# --- OOD Test Config ---\n",
    "OOD_TEST_SAMPLES_PER_CLASS = {1: 100, 2: 100, 3: 100, 4: 100}\n",
    "\n",
    "# --- Single root output directory (each model writes under its subfolder) ---\n",
    "BASE_OUTPUT_DIR = \"./outputs\"  # final path: ./outputs/<model_name_short>/\n",
    "# ########################################################################\n",
    "\n",
    "\n",
    "# --- Helper Classes (Probe Definitions) ---\n",
    "class PyTorchMLPProbe(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, in_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# --- Function Definitions ---\n",
    "def setup_environment(seed, device_id):\n",
    "    \"\"\"Set random seeds and choose computation device.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        device = torch.device(f\"cuda:{device_id}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name, device):\n",
    "    \"\"\"Load pretrained model and tokenizer.\"\"\"\n",
    "    logger.info(f\"Loading model: {model_name} ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=torch.bfloat16,  # adjust if needed\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def resolve_layers_to_probe(model, tokenizer, device,\n",
    "                            include_embedding=False, every_k=1, limit=None,\n",
    "                            sample_text=\"Hello\"):\n",
    "    \"\"\"\n",
    "    Run a minimal forward pass to read the length of hidden_states and return\n",
    "    the indices to probe.\n",
    "    - If include_embedding=False, start from index 1 (Transformer block outputs only)\n",
    "    - every_k: sample every k-th layer\n",
    "    - limit: keep only the first N indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        n_h = len(outputs.hidden_states)  # typically num_hidden_layers + 1\n",
    "    start = 0 if include_embedding else 1\n",
    "    indices = list(range(start, n_h))\n",
    "    if every_k and every_k > 1:\n",
    "        indices = indices[::every_k]\n",
    "    if limit is not None:\n",
    "        indices = indices[:limit]\n",
    "    logger.info(f\"[Auto] hidden_states length = {n_h}; probing indices = {indices}\")\n",
    "    return indices\n",
    "\n",
    "\n",
    "def generate_3way_commutativity_data(n_samples_per_class, seed):\n",
    "    \"\"\"Generate training data for 3-way commutativity probing.\"\"\"\n",
    "    logger.info(f\"--- Generating 3-Way Commutativity Data ---\")\n",
    "    logger.info(f\"Target: {n_samples_per_class} samples per class (a+b (a>b), b+a (a>b), a+a)\")\n",
    "    _random = random.Random(seed)\n",
    "    samples_class_0, samples_class_1, samples_class_2 = [], [], []\n",
    "\n",
    "    # Class 2 (a == b)\n",
    "    max_attempts_c2 = n_samples_per_class * 5\n",
    "    count_c2 = 0\n",
    "    while len(samples_class_2) < n_samples_per_class and count_c2 < max_attempts_c2:\n",
    "        a = _random.randint(10, 99)\n",
    "        prompt = f\"Calculate: {a}+{a} = \"\n",
    "        samples_class_2.append({'prompt': prompt, 'label': 2, 'a': a, 'b': a})\n",
    "        count_c2 += 1\n",
    "    if count_c2 >= max_attempts_c2:\n",
    "        logger.warning(\"Max attempts reached generating Class 2 samples.\")\n",
    "\n",
    "    # Class 0/1 (a > b)\n",
    "    generated_pairs_c01 = 0\n",
    "    max_attempts_c01 = n_samples_per_class * 10\n",
    "    while generated_pairs_c01 < n_samples_per_class and generated_pairs_c01 * 2 < max_attempts_c01:\n",
    "        a = _random.randint(10, 99)\n",
    "        b = _random.randint(10, 99)\n",
    "        if a == b:\n",
    "            continue\n",
    "        if a < b:\n",
    "            a, b = b, a\n",
    "        prompt0 = f\"Calculate: {a}+{b} = \"\n",
    "        samples_class_0.append({'prompt': prompt0, 'label': 0, 'a': a, 'b': b})\n",
    "        prompt1 = f\"Calculate: {b}+{a} = \"\n",
    "        samples_class_1.append({'prompt': prompt1, 'label': 1, 'a': a, 'b': b})\n",
    "        generated_pairs_c01 += 1\n",
    "    if generated_pairs_c01 * 2 >= max_attempts_c01:\n",
    "        logger.warning(\"Max attempts reached generating Class 0/1 samples.\")\n",
    "\n",
    "    all_samples = samples_class_0 + samples_class_1 + samples_class_2\n",
    "    _random.shuffle(all_samples)\n",
    "    logger.info(f\"Generated final dataset with {len(all_samples)} samples.\")\n",
    "    if all_samples:\n",
    "        all_labels_list = [s['label'] for s in all_samples]\n",
    "        final_counts_dict = {i: all_labels_list.count(i) for i in range(OUTPUT_DIM_3WAY)}\n",
    "        logger.info(f\"Final label distribution: {final_counts_dict}\")\n",
    "    else:\n",
    "        raise ValueError(\"No 3-WAY COMMUTATIVITY samples generated.\")\n",
    "    return all_samples\n",
    "\n",
    "\n",
    "def get_activations(model, tokenizer, prompts, layers_to_probe, device,\n",
    "                    batch_size=32, desc_prefix=\"\"):\n",
    "    \"\"\"Get hidden state activations (last token) for specified layers and inputs.\"\"\"\n",
    "    logger.info(f\"[{desc_prefix}] Extracting activations for {len(prompts)} prompts, layers={layers_to_probe}\")\n",
    "    activations = {layer: [] for layer in layers_to_probe}\n",
    "    model.eval()\n",
    "    iterable = range(0, len(prompts), batch_size)\n",
    "    iterator = tqdm(iterable, desc=f\"{desc_prefix} Extract\", disable=not USE_TQDM)\n",
    "    with torch.no_grad():\n",
    "        for i in iterator:\n",
    "            batch_prompts = prompts[i: i + batch_size]\n",
    "            try:\n",
    "                inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True,\n",
    "                                   truncation=True, max_length=64).to(device)\n",
    "                outputs = model(**inputs)\n",
    "                hidden_states = outputs.hidden_states\n",
    "                sequence_lengths = inputs.attention_mask.sum(dim=1)\n",
    "                last_token_indices = sequence_lengths - 1\n",
    "                for layer_idx in layers_to_probe:\n",
    "                    if not (0 <= layer_idx < len(hidden_states)):\n",
    "                        continue\n",
    "                    layer_hidden_states = hidden_states[layer_idx]\n",
    "                    last_token_activations = layer_hidden_states[\n",
    "                        torch.arange(layer_hidden_states.size(0)), last_token_indices, :\n",
    "                    ]\n",
    "                    activations[layer_idx].append(\n",
    "                        last_token_activations.to(torch.float32).cpu().numpy()\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Error processing batch at index {i} ({desc_prefix}): {e}\")\n",
    "\n",
    "    final_activations = {}\n",
    "    for layer in layers_to_probe:\n",
    "        if activations[layer]:\n",
    "            try:\n",
    "                final_activations[layer] = np.concatenate(activations[layer], axis=0)\n",
    "            except ValueError as e:\n",
    "                logger.exception(f\"Error concatenating for layer {layer} ({desc_prefix}): {e}\")\n",
    "                final_activations[layer] = np.array([])\n",
    "        else:\n",
    "            final_activations[layer] = np.array([])\n",
    "    logger.info(f\"[{desc_prefix}] Finished extracting activations.\")\n",
    "    return final_activations\n",
    "\n",
    "\n",
    "def train_and_evaluate_probes(\n",
    "        all_samples, extracted_activations, layers_to_probe, device,\n",
    "        log_filename_base=\"probe_log\", output_dir=\"./outputs\",\n",
    "        num_epochs=NUM_EPOCHS_PROBE, batch_size=BATCH_SIZE_PROBE, learning_rate=LEARNING_RATE_PROBE,\n",
    "        test_split_size=TEST_SPLIT_SIZE, random_seed=42, output_dim=OUTPUT_DIM_3WAY\n",
    "):\n",
    "    \"\"\"Train, evaluate, and save a probe per layer; return evaluation results.\"\"\"\n",
    "    y_all = np.array([s['label'] for s in all_samples])\n",
    "    results = {\"layer\": [], \"accuracy\": [], \"report_dict\": [], \"f1_class0\": [], \"f1_class1\": [], \"f1_class2\": []}\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    experiment_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    per_run_report_path = os.path.join(output_dir, f\"{log_filename_base}_{experiment_timestamp}.txt\")\n",
    "    logger.info(f\"Per-layer classification reports -> {per_run_report_path}\")\n",
    "\n",
    "    with open(per_run_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Output dir: {output_dir}\\n\")\n",
    "        f.write(f\"Layers: {layers_to_probe}\\n\")\n",
    "        f.write(f\"Num samples: {len(all_samples)}\\n\\n\")\n",
    "\n",
    "    logger.info(f\"--- Starting Probe Training Loop for layers {layers_to_probe[0]}..{layers_to_probe[-1]} ---\")\n",
    "\n",
    "    for layer in tqdm(layers_to_probe, desc=\"Probe train/eval\", disable=not USE_TQDM):\n",
    "        logger.info(f\"--- Processing Layer {layer} ---\")\n",
    "        if layer not in extracted_activations or extracted_activations[layer].shape[0] == 0:\n",
    "            logger.warning(f\"Skipping layer {layer}: No activation data.\")\n",
    "            results[\"layer\"].append(layer)\n",
    "            results[\"accuracy\"].append(np.nan)\n",
    "            results[\"report_dict\"].append({})\n",
    "            results[\"f1_class0\"].append(np.nan)\n",
    "            results[\"f1_class1\"].append(np.nan)\n",
    "            results[\"f1_class2\"].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        X_all_layer = extracted_activations[layer]\n",
    "        if X_all_layer.shape[0] != len(y_all):\n",
    "            logger.warning(f\"Skipping layer {layer}: Mismatch activations ({X_all_layer.shape[0]}) / labels ({len(y_all)}).\")\n",
    "            results[\"layer\"].append(layer)\n",
    "            results[\"accuracy\"].append(np.nan)\n",
    "            results[\"report_dict\"].append({})\n",
    "            results[\"f1_class0\"].append(np.nan)\n",
    "            results[\"f1_class1\"].append(np.nan)\n",
    "            results[\"f1_class2\"].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_all_layer, y_all, test_size=test_split_size, random_state=random_seed, stratify=y_all\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            logger.exception(f\"Skipping layer {layer}: Error during train/test split: {e}\")\n",
    "            results[\"layer\"].append(layer)\n",
    "            results[\"accuracy\"].append(np.nan)\n",
    "            results[\"report_dict\"].append({})\n",
    "            results[\"f1_class0\"].append(np.nan)\n",
    "            results[\"f1_class1\"].append(np.nan)\n",
    "            results[\"f1_class2\"].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        input_dim_probe = X_train.shape[1]\n",
    "        probe = LinearProbe(input_dim_probe, output_dim).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(probe.parameters(), lr=learning_rate)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Train\n",
    "        probe.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = probe(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        logger.info(f\"Layer {layer} - Training completed.\")\n",
    "\n",
    "        # Save probe state_dict\n",
    "        layer_probe_dir = os.path.join(output_dir, f\"layer_{layer}\")\n",
    "        os.makedirs(layer_probe_dir, exist_ok=True)\n",
    "        state_dict_path = os.path.join(layer_probe_dir, \"state_dict.pt\")\n",
    "        torch.save(probe.state_dict(), state_dict_path)\n",
    "        logger.info(f\"Saved probe state_dict -> {state_dict_path}\")\n",
    "\n",
    "        # Eval\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = probe(X_test_tensor)\n",
    "            _, predicted_indices = torch.max(test_outputs, 1)\n",
    "            y_pred_np = predicted_indices.cpu().numpy()\n",
    "            y_true_np = y_test_tensor.cpu().numpy()\n",
    "\n",
    "            acc_test = accuracy_score(y_true_np, y_pred_np)\n",
    "            target_names = ['a+b (a>b)', 'b+a (a>b)', 'a+a (a=b)']\n",
    "\n",
    "            report_dict_sklearn = classification_report(\n",
    "                y_true_np, y_pred_np, labels=list(range(output_dim)),\n",
    "                target_names=target_names, digits=3, zero_division=0, output_dict=True\n",
    "            )\n",
    "            report_str_sklearn = classification_report(\n",
    "                y_true_np, y_pred_np, labels=list(range(output_dim)),\n",
    "                target_names=target_names, digits=3, zero_division=0, output_dict=False\n",
    "            )\n",
    "\n",
    "            f1_c0 = report_dict_sklearn.get(target_names[0], {}).get('f1-score', np.nan)\n",
    "            f1_c1 = report_dict_sklearn.get(target_names[1], {}).get('f1-score', np.nan)\n",
    "            f1_c2 = report_dict_sklearn.get(target_names[2], {}).get('f1-score', np.nan)\n",
    "\n",
    "        results[\"layer\"].append(layer)\n",
    "        results[\"accuracy\"].append(acc_test)\n",
    "        results[\"report_dict\"].append(report_dict_sklearn)\n",
    "        results[\"f1_class0\"].append(f1_c0)\n",
    "        results[\"f1_class1\"].append(f1_c1)\n",
    "        results[\"f1_class2\"].append(f1_c2)\n",
    "\n",
    "        logger.info(f\"Layer {layer} - Test Acc: {acc_test:.4f} | F1(C0,C1,C2): {f1_c0:.3f}, {f1_c1:.3f}, {f1_c2:.3f}\")\n",
    "\n",
    "        with open(per_run_report_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"--- Layer {layer} ---\\n\")\n",
    "            f.write(f\"Test Accuracy: {acc_test:.4f}\\n\")\n",
    "            f.write(f\"F1-Score Class 0: {f1_c0:.4f}\\n\")\n",
    "            f.write(f\"F1-Score Class 1: {f1_c1:.4f}\\n\")\n",
    "            f.write(f\"F1-Score Class 2: {f1_c2:.4f}\\n\")\n",
    "            f.write(f\"Classification Report:\\n{report_str_sklearn}\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "    logger.info(\"--- Probe Training Loop Finished ---\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_ood_comm_samples(n_digits, n_per_class, seed, exclude_prompts=None):\n",
    "    \"\"\"Generate OOD samples; optionally exclude specific prompts.\"\"\"\n",
    "    _local_rng = random.Random(seed)\n",
    "    samples_0, samples_1, samples_2 = [], [], []\n",
    "    low, high = (10 ** (n_digits - 1), 10 ** n_digits - 1) if n_digits > 1 else (1, 9)\n",
    "\n",
    "    # Class 2 (a==b)\n",
    "    attempts_c2 = 0\n",
    "    max_attempts_c2_ood = n_per_class * 50\n",
    "    while len(samples_2) < n_per_class and attempts_c2 < max_attempts_c2_ood:\n",
    "        a = _local_rng.randint(low, high)\n",
    "        p = f\"Calculate: {a}+{a} = \"\n",
    "        if exclude_prompts and p in exclude_prompts:\n",
    "            attempts_c2 += 1\n",
    "            continue\n",
    "        samples_2.append(dict(prompt=p, label=2, a=a, b=a))\n",
    "        if exclude_prompts:\n",
    "            exclude_prompts.add(p)\n",
    "        attempts_c2 = 0\n",
    "    if len(samples_2) < n_per_class:\n",
    "        logger.warning(f\"[WARN] OOD Class-2 ({n_digits}-digit): Got {len(samples_2)}/{n_per_class}\")\n",
    "\n",
    "    # Class 0/1\n",
    "    attempts_c01 = 0\n",
    "    max_attempts_c01_ood = n_per_class * 50\n",
    "    while len(samples_0) < n_per_class and attempts_c01 < max_attempts_c01_ood:\n",
    "        a, b = _local_rng.randint(low, high), _local_rng.randint(low, high)\n",
    "        if a == b:\n",
    "            attempts_c01 += 1\n",
    "            continue\n",
    "        if a < b:\n",
    "            a, b = b, a\n",
    "        p0, p1 = f\"Calculate: {a}+{b} = \", f\"Calculate: {b}+{a} = \"\n",
    "        if exclude_prompts and (p0 in exclude_prompts or p1 in exclude_prompts):\n",
    "            attempts_c01 += 1\n",
    "            continue\n",
    "        samples_0.append(dict(prompt=p0, label=0, a=a, b=b))\n",
    "        samples_1.append(dict(prompt=p1, label=1, a=a, b=b))\n",
    "        if exclude_prompts:\n",
    "            exclude_prompts.update([p0, p1])\n",
    "        attempts_c01 = 0\n",
    "    if len(samples_0) < n_per_class:\n",
    "        logger.warning(f\"[WARN] OOD Class-0/1 ({n_digits}-digit): Got {len(samples_0)}/{n_per_class}\")\n",
    "\n",
    "    return samples_0 + samples_1 + samples_2\n",
    "\n",
    "\n",
    "def evaluate_probe_on_ood_layer(model_llm, tokenizer_llm, layer_idx, probe_model,\n",
    "                                prompts, labels, device, batch_size_activations):\n",
    "    \"\"\"Evaluate a specific layer's probe on OOD data.\"\"\"\n",
    "    activations_ood_layer = get_activations(\n",
    "        model_llm, tokenizer_llm, prompts, [layer_idx], device,\n",
    "        batch_size_activations, desc_prefix=f\"OOD_L{layer_idx}\"\n",
    "    )\n",
    "    if layer_idx not in activations_ood_layer or activations_ood_layer[layer_idx].shape[0] == 0:\n",
    "        logger.warning(f\"Could not get OOD activations for layer {layer_idx}\")\n",
    "        return np.nan, \"No OOD activations\"\n",
    "\n",
    "    X_ood = activations_ood_layer[layer_idx]\n",
    "    if X_ood.shape[0] != len(labels):\n",
    "        logger.warning(f\"Mismatch OOD activations ({X_ood.shape[0]}) and labels ({len(labels)}) for layer {layer_idx}\")\n",
    "        return np.nan, \"Mismatch OOD activations/labels\"\n",
    "\n",
    "    X_ood_t = torch.tensor(X_ood, dtype=torch.float32).to(device)\n",
    "    probe_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_ood = probe_model(X_ood_t).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(labels, y_pred_ood)\n",
    "    report = classification_report(labels, y_pred_ood, labels=[0, 1, 2],\n",
    "                                   target_names=['a+b', 'b+a', 'a+a'],\n",
    "                                   zero_division=0, digits=3, output_dict=False)\n",
    "    return acc, report\n",
    "\n",
    "\n",
    "def evaluate_all_probes_on_ood(model_llm, tokenizer_llm, layers_to_probe, probes_dir_for_run,\n",
    "                               ood_datasets_map, device, input_dim_probes,\n",
    "                               output_dim_probes=OUTPUT_DIM_3WAY,\n",
    "                               batch_size_activations=BATCH_SIZE_PROBE):\n",
    "    \"\"\"Evaluate all saved probes across all OOD datasets (one pass per dataset).\"\"\"\n",
    "    logger.info(\"--- Evaluating Probes on OOD Datasets (one pass per dataset) ---\")\n",
    "\n",
    "    # 1) Preload all layer probes to reduce repeated I/O\n",
    "    probes = {}\n",
    "    for layer in layers_to_probe:\n",
    "        probe_path = os.path.join(probes_dir_for_run, f\"layer_{layer}\", \"state_dict.pt\")\n",
    "        if not os.path.exists(probe_path):\n",
    "            logger.warning(f\"Probe for layer {layer} missing at {probe_path}\")\n",
    "            continue\n",
    "        probe = LinearProbe(input_dim_probes, output_dim_probes).to(device)\n",
    "        probe.load_state_dict(torch.load(probe_path, map_location=device))\n",
    "        probe.eval()\n",
    "        probes[layer] = probe\n",
    "\n",
    "    ood_summary = defaultdict(list)\n",
    "    ood_summary['layer'] = layers_to_probe\n",
    "\n",
    "    # 2) For each OOD dataset: extract activations for all layers in a single pass\n",
    "    for digits, ds_ood in ood_datasets_map.items():\n",
    "        prompts_ood = [s['prompt'] for s in ds_ood]\n",
    "        labels_ood = np.array([s['label'] for s in ds_ood])\n",
    "\n",
    "        acts_by_layer = get_activations(\n",
    "            model_llm, tokenizer_llm, prompts_ood, layers_to_probe, device,\n",
    "            batch_size=batch_size_activations, desc_prefix=f\"OOD_{digits}d_ALL\"\n",
    "        )  # key: one-time extraction for all layers\n",
    "\n",
    "        acc_key = f\"acc_{digits}d\"\n",
    "        for layer in layers_to_probe:\n",
    "            if layer not in probes or layer not in acts_by_layer or acts_by_layer[layer].size == 0:\n",
    "                ood_summary[acc_key].append(float('nan'))\n",
    "                continue\n",
    "            X = torch.tensor(acts_by_layer[layer], dtype=torch.float32, device=device)\n",
    "            with torch.no_grad():\n",
    "                pred = probes[layer](X).argmax(dim=1).cpu().numpy()\n",
    "            acc = (pred == labels_ood).mean()\n",
    "            ood_summary[acc_key].append(float(acc))\n",
    "            logger.info(f\"[OOD {digits}d] Layer {layer}: acc={acc:.4f}\")\n",
    "\n",
    "    return ood_summary\n",
    "\n",
    "\n",
    "\n",
    "def generate_final_comparison_plots_with_ci(\n",
    "        all_runs_results_in_domain,   # list of results_3way_in_domain dicts\n",
    "        all_runs_ood_summaries,       # list of ood_evaluation_summary dicts\n",
    "        layers_plotted_config,        # e.g., per-model LAYERS_TO_PROBE\n",
    "        output_dir,\n",
    "        plot_timestamp,\n",
    "        model_name_short,\n",
    "        n_replicates\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate final in-domain vs OOD performance comparison plots (line + bar)\n",
    "    with approximate 95% CIs, and save to a unified folder per model.\n",
    "    Note: layers_plotted_config is the model's own layer index list; each model\n",
    "    is plotted and saved separately.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Generating Final Aggregated Plots (with CI) ---\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    num_layers = len(layers_plotted_config)\n",
    "\n",
    "    # Initialize aggregation containers\n",
    "    acc_2d_in_domain_runs = np.full((n_replicates, num_layers), np.nan)\n",
    "    acc_1d_ood_runs = np.full((n_replicates, num_layers), np.nan)\n",
    "    acc_2d_ood_runs = np.full((n_replicates, num_layers), np.nan)\n",
    "    acc_3d_ood_runs = np.full((n_replicates, num_layers), np.nan)\n",
    "    acc_4d_ood_runs = np.full((n_replicates, num_layers), np.nan)\n",
    "\n",
    "    mean_acc_2d_in_domain_bar_runs = np.full(n_replicates, np.nan)\n",
    "    mean_acc_1d_ood_bar_runs = np.full(n_replicates, np.nan)\n",
    "    mean_acc_2d_ood_bar_runs = np.full(n_replicates, np.nan)\n",
    "    mean_acc_3d_ood_bar_runs = np.full(n_replicates, np.nan)\n",
    "    mean_acc_4d_ood_bar_runs = np.full(n_replicates, np.nan)\n",
    "\n",
    "    # Fill data from each run\n",
    "    for i_run in range(n_replicates):\n",
    "        res_in_domain_run = all_runs_results_in_domain[i_run]\n",
    "        layer_to_idx_map_in_domain = {l: idx for idx, l in enumerate(res_in_domain_run['layer'])}\n",
    "\n",
    "        current_run_in_domain_accs = []\n",
    "        for layer_config_idx, layer_val in enumerate(layers_plotted_config):\n",
    "            if layer_val in layer_to_idx_map_in_domain:\n",
    "                source_idx = layer_to_idx_map_in_domain[layer_val]\n",
    "                acc = res_in_domain_run['accuracy'][source_idx]\n",
    "                acc_2d_in_domain_runs[i_run, layer_config_idx] = acc\n",
    "                if not np.isnan(acc):\n",
    "                    current_run_in_domain_accs.append(acc)\n",
    "        if current_run_in_domain_accs:\n",
    "            mean_acc_2d_in_domain_bar_runs[i_run] = np.nanmean(current_run_in_domain_accs)\n",
    "\n",
    "        # OOD\n",
    "        ood_summary_run = all_runs_ood_summaries[i_run]\n",
    "        layer_to_idx_map_ood = {l: idx for idx, l in enumerate(ood_summary_run['layer'])}\n",
    "        current_run_ood_accs = {'1d': [], '2d': [], '3d': [], '4d': []}\n",
    "        for layer_config_idx, layer_val in enumerate(layers_plotted_config):\n",
    "            if layer_val in layer_to_idx_map_ood:\n",
    "                source_idx = layer_to_idx_map_ood[layer_val]\n",
    "                if source_idx < len(ood_summary_run.get('acc_1d', [])):\n",
    "                    acc_1d = ood_summary_run['acc_1d'][source_idx]\n",
    "                    acc_1d_ood_runs[i_run, layer_config_idx] = acc_1d\n",
    "                    if not np.isnan(acc_1d): current_run_ood_accs['1d'].append(acc_1d)\n",
    "                if source_idx < len(ood_summary_run.get('acc_2d', [])):\n",
    "                    acc_2d = ood_summary_run['acc_2d'][source_idx]\n",
    "                    acc_2d_ood_runs[i_run, layer_config_idx] = acc_2d\n",
    "                    if not np.isnan(acc_2d): current_run_ood_accs['2d'].append(acc_2d)\n",
    "                if source_idx < len(ood_summary_run.get('acc_3d', [])):\n",
    "                    acc_3d = ood_summary_run['acc_3d'][source_idx]\n",
    "                    acc_3d_ood_runs[i_run, layer_config_idx] = acc_3d\n",
    "                    if not np.isnan(acc_3d): current_run_ood_accs['3d'].append(acc_3d)\n",
    "                if source_idx < len(ood_summary_run.get('acc_4d', [])):\n",
    "                    acc_4d = ood_summary_run['acc_4d'][source_idx]\n",
    "                    acc_4d_ood_runs[i_run, layer_config_idx] = acc_4d\n",
    "                    if not np.isnan(acc_4d): current_run_ood_accs['4d'].append(acc_4d)\n",
    "\n",
    "        if current_run_ood_accs['1d']: mean_acc_1d_ood_bar_runs[i_run] = np.nanmean(current_run_ood_accs['1d'])\n",
    "        if current_run_ood_accs['2d']: mean_acc_2d_ood_bar_runs[i_run] = np.nanmean(current_run_ood_accs['2d'])\n",
    "        if current_run_ood_accs['3d']: mean_acc_3d_ood_bar_runs[i_run] = np.nanmean(current_run_ood_accs['3d'])\n",
    "        if current_run_ood_accs['4d']: mean_acc_4d_ood_bar_runs[i_run] = np.nanmean(current_run_ood_accs['4d'])\n",
    "\n",
    "    # Stats: mean and approx 95% CI\n",
    "    def get_mean_and_ci(data_runs_per_layer, confidence=0.95):\n",
    "        mean_per_layer = np.nanmean(data_runs_per_layer, axis=0)\n",
    "        sem_per_layer = stats.sem(data_runs_per_layer, axis=0, nan_policy='omit')\n",
    "        sem_per_layer = np.nan_to_num(sem_per_layer)\n",
    "        h_per_layer = 1.96 * sem_per_layer\n",
    "        return mean_per_layer, np.clip(mean_per_layer - h_per_layer, 0, 1), np.clip(mean_per_layer + h_per_layer, 0, 1)\n",
    "\n",
    "    mean_2d_in, ci_low_2d_in, ci_high_2d_in = get_mean_and_ci(acc_2d_in_domain_runs)\n",
    "    mean_1d_ood, ci_low_1d_ood, ci_high_1d_ood = get_mean_and_ci(acc_1d_ood_runs)\n",
    "    mean_2d_ood, ci_low_2d_ood, ci_high_2d_ood = get_mean_and_ci(acc_2d_ood_runs)\n",
    "    mean_3d_ood, ci_low_3d_ood, ci_high_3d_ood = get_mean_and_ci(acc_3d_ood_runs)\n",
    "    mean_4d_ood, ci_low_4d_ood, ci_high_4d_ood = get_mean_and_ci(acc_4d_ood_runs)\n",
    "\n",
    "    layers_for_plot = np.array(layers_plotted_config)\n",
    "\n",
    "    # Matplotlib style\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 14,\n",
    "        'font.family': 'serif',\n",
    "        'font.sans-serif': ['Arial'],\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 20,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 14,\n",
    "        'lines.linewidth': 1.8,\n",
    "        'lines.markersize': 6,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.5,\n",
    "        'grid.linestyle': ':',\n",
    "        'savefig.dpi': 600,\n",
    "        'savefig.format': 'pdf',\n",
    "        'savefig.bbox': 'tight',\n",
    "    })\n",
    "    colors = plt.cm.get_cmap('tab10', 5)\n",
    "    color_map = {\n",
    "        '2-digit_in': colors(0), '1-digit_ood': colors(1), '2-digit_ood': colors(2),\n",
    "        '3-digit_ood': colors(3), '4-digit_ood': colors(4), 'chance': 'grey'\n",
    "    }\n",
    "    marker_map = {'2-digit_in': 'o', '1-digit_ood': '^', '2-digit_ood': 'd',\n",
    "                  '3-digit_ood': 's', '4-digit_ood': 'v'}\n",
    "\n",
    "    # Line plot\n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 5.5))\n",
    "\n",
    "    def plot_line_with_ci(ax, x, mean, ci_low, ci_high, color, marker, label):\n",
    "        ax.plot(x, mean, marker=marker, label=label, color=color)\n",
    "        ax.fill_between(x, ci_low, ci_high, color=color, alpha=0.2)\n",
    "\n",
    "    plot_line_with_ci(ax1, layers_for_plot, mean_2d_in, ci_low_2d_in, ci_high_2d_in,\n",
    "                      color_map['2-digit_in'], marker_map['2-digit_in'], '2-digit (In-Domain Train)')\n",
    "    ax1.plot(layers_for_plot, mean_2d_ood, marker=marker_map['2-digit_ood'],\n",
    "             label='2-digit (OOD Test)', color=color_map['2-digit_ood'], linestyle='--')\n",
    "    ax1.fill_between(layers_for_plot, ci_low_2d_ood, ci_high_2d_ood, color=color_map['2-digit_ood'], alpha=0.2)\n",
    "    plot_line_with_ci(ax1, layers_for_plot, mean_1d_ood, ci_low_1d_ood, ci_high_1d_ood,\n",
    "                      color_map['1-digit_ood'], marker_map['1-digit_ood'], '1-digit (OOD Test)')\n",
    "    plot_line_with_ci(ax1, layers_for_plot, mean_3d_ood, ci_low_3d_ood, ci_high_3d_ood,\n",
    "                      color_map['3-digit_ood'], marker_map['3-digit_ood'], '3-digit (OOD Test)')\n",
    "    plot_line_with_ci(ax1, layers_for_plot, mean_4d_ood, ci_low_4d_ood, ci_high_4d_ood,\n",
    "                      color_map['4-digit_ood'], marker_map['4-digit_ood'], '4-digit (OOD Test)')\n",
    "\n",
    "    ax1.axhline(y=1.0 / 3, linestyle='--', linewidth=1.0, label='Chance (0.333)',\n",
    "                color=color_map['chance'], zorder=0)\n",
    "    model_tag = sanitize_model_name(model_id)  \n",
    "    ax1.set_xlabel(f\"Layer Index({model_tag})\")\n",
    "    ax1.set_ylabel(\"Mean Accuracy\")\n",
    "    ax1.set_title(f\"Probe Generalisation Across Digit Lengths ({n_replicates} Runs)\")\n",
    "    ax1.set_xticks(layers_for_plot)\n",
    "    ax1.set_ylim(0.0, 1.05)\n",
    "    ax1.legend(loc='lower right', fontsize=12, frameon=True,\n",
    "               facecolor='white', framealpha=0.8, borderpad=0.2,\n",
    "               labelspacing=0.2, handlelength=1.0, handletextpad=0.4)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.tick_params(direction='in', top=False, right=False)\n",
    "    ax1.minorticks_on()\n",
    "    ax1.grid(True, linestyle=':', linewidth=0.4, which='minor', alpha=0.4, axis='y')\n",
    "    ax1.grid(True, linestyle=':', linewidth=0.6, which='major', alpha=0.7, axis='y')\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    pdf_line_path = os.path.join(output_dir, f\"AGG_probe_comm_generalisation_line_{model_name_short}_{plot_timestamp}.pdf\")\n",
    "    plt.savefig(pdf_line_path)\n",
    "    plt.close(fig1)\n",
    "    logger.info(f\"Saved aggregated per-layer line chart (PDF) -> {pdf_line_path}\")\n",
    "\n",
    "    # Bar chart\n",
    "    bar_means = {\n",
    "        \"2-digit (Train)\": np.nanmean(np.nanmean(acc_2d_in_domain_runs, axis=1)),\n",
    "        \"1-digit (OOD)\": np.nanmean(np.nanmean(acc_1d_ood_runs, axis=1)),\n",
    "        \"2-digit (OOD)\": np.nanmean(np.nanmean(acc_2d_ood_runs, axis=1)),\n",
    "        \"3-digit (OOD)\": np.nanmean(np.nanmean(acc_3d_ood_runs, axis=1)),\n",
    "        \"4-digit (OOD)\": np.nanmean(np.nanmean(acc_4d_ood_runs, axis=1)),\n",
    "    }\n",
    "    bar_sems = {\n",
    "        \"2-digit (Train)\": stats.sem(np.nanmean(acc_2d_in_domain_runs, axis=1), nan_policy='omit'),\n",
    "        \"1-digit (OOD)\": stats.sem(np.nanmean(acc_1d_ood_runs, axis=1), nan_policy='omit'),\n",
    "        \"2-digit (OOD)\": stats.sem(np.nanmean(acc_2d_ood_runs, axis=1), nan_policy='omit'),\n",
    "        \"3-digit (OOD)\": stats.sem(np.nanmean(acc_3d_ood_runs, axis=1), nan_policy='omit'),\n",
    "        \"4-digit (OOD)\": stats.sem(np.nanmean(acc_4d_ood_runs, axis=1), nan_policy='omit'),\n",
    "    }\n",
    "    for k in bar_sems:\n",
    "        bar_sems[k] = np.nan_to_num(bar_sems[k])\n",
    "\n",
    "    labels_bar = list(bar_means.keys())\n",
    "    means_bar_values = [bar_means[k] for k in labels_bar]\n",
    "    sems_bar_values = [1.96 * bar_sems[k] for k in labels_bar]\n",
    "\n",
    "    colors_map = plt.cm.get_cmap('tab10', 5)\n",
    "    bar_colors_map_list = [colors_map(0), colors_map(1), colors_map(2), colors_map(3), colors_map(4)]\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(7, 5.5))\n",
    "    bars = ax2.bar(range(len(labels_bar)), means_bar_values, yerr=sems_bar_values,\n",
    "                   color=bar_colors_map_list, width=0.6, edgecolor='black', linewidth=0.7, capsize=5)\n",
    "    ax2.set_xticks(range(len(labels_bar)))\n",
    "    ax2.set_xticklabels(labels_bar, rotation=25, ha=\"right\")\n",
    "    ax2.axhline(y=1.0 / 3, linestyle='--', linewidth=1.2, color='grey', zorder=0)\n",
    "    ax2.set_ylabel(\"Mean Accuracy (Averaged Across Layers & Runs)\")\n",
    "    ymax = max([v for v in means_bar_values if not np.isnan(v)] + [0.0])\n",
    "    ax2.set_ylim(0, ymax * 1.15 if ymax > 0 else 1.0)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2.0, yval + 0.005, f'{yval:.3f}',\n",
    "                 va='bottom', ha='center', fontsize=12)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.tick_params(direction='in', top=False, right=False, bottom=False)\n",
    "    ax2.yaxis.grid(True, linestyle=':', linewidth=0.6, alpha=0.7)\n",
    "    ax2.set_axisbelow(True)\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    pdf_bar_path = os.path.join(output_dir, f\"AGG_probe_comm_generalisation_bar_{model_name_short}_{plot_timestamp}.pdf\")\n",
    "    plt.savefig(pdf_bar_path)\n",
    "    plt.close(fig2)\n",
    "    logger.info(f\"Saved aggregated mean-accuracy bar chart (PDF) -> {pdf_bar_path}\")\n",
    "\n",
    "\n",
    "# ========== Single-model main routine ==========\n",
    "\n",
    "def run_experiment_for_model(model_id: str):\n",
    "    \"\"\"Run the full pipeline for a single model: load, auto-select layers, train N runs,\n",
    "    OOD evaluation, and aggregated plotting.\n",
    "    \"\"\"\n",
    "    # Timestamp & short model name\n",
    "    RUN_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name_short = os.path.basename(model_id.strip('/')) or re.sub(r'[^A-Za-z0-9_.-]+', '_', model_id)\n",
    "\n",
    "    # Unified output directory (per model)\n",
    "    OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, model_name_short)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Main log goes under the unified output directory\n",
    "    LOG_PATH = os.path.join(OUTPUT_DIR, f\"run_{RUN_TIMESTAMP}.log\")\n",
    "    setup_logger(LOG_PATH, also_console=False)\n",
    "\n",
    "    # 1) Environment & model (init with the first seed)\n",
    "    temp_device = setup_environment(RANDOM_SEEDS_LIST[0], DEVICE_ID)\n",
    "    try:\n",
    "        model_llm, tokenizer_llm = load_model_and_tokenizer(model_id, temp_device)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"[FATAL] Failed to load model {model_id}: {e}\")\n",
    "        return  # do not stop other models\n",
    "\n",
    "    # 2) Dynamically determine layers_to_probe (per model)\n",
    "    layers_to_probe = resolve_layers_to_probe(\n",
    "        model_llm, tokenizer_llm, temp_device,\n",
    "        include_embedding=AUTO_INCLUDE_EMBEDDING_STATE,\n",
    "        every_k=AUTO_EVERY_K_LAYERS,\n",
    "        limit=AUTO_LIMIT_MAX_LAYERS,\n",
    "        sample_text=\"Hello\"\n",
    "    )\n",
    "    if not isinstance(layers_to_probe, (list, tuple)) or len(layers_to_probe) == 0:\n",
    "        logger.error(\"LAYERS_TO_PROBE is empty; skipping this model.\")\n",
    "        try:\n",
    "            del model_llm; del tokenizer_llm\n",
    "        except Exception:\n",
    "            pass\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return\n",
    "    logger.info(f\"Final LAYERS_TO_PROBE for {model_name_short}: {layers_to_probe}\")\n",
    "\n",
    "    # 3) Determine probe input dimension (with a small sample)\n",
    "    logger.info(\"Determining probe input dimension...\")\n",
    "    _placeholder_samples = generate_3way_commutativity_data(10, RANDOM_SEEDS_LIST[0])\n",
    "    _placeholder_prompts = [s['prompt'] for s in _placeholder_samples]\n",
    "    _placeholder_activations = get_activations(\n",
    "        model_llm, tokenizer_llm, _placeholder_prompts, [layers_to_probe[0]],\n",
    "        temp_device, batch_size=10, desc_prefix=\"PLACEHOLDER\"\n",
    "    )\n",
    "    if layers_to_probe[0] not in _placeholder_activations or _placeholder_activations[layers_to_probe[0]].size == 0:\n",
    "        logger.error(\"Could not get placeholder activations to determine probe input dimension. Skip model.\")\n",
    "        try:\n",
    "            del model_llm; del tokenizer_llm\n",
    "        except Exception:\n",
    "            pass\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return\n",
    "    probe_input_dimension_fixed = _placeholder_activations[layers_to_probe[0]].shape[1]\n",
    "    logger.info(f\"Determined fixed probe input dimension: {probe_input_dimension_fixed}\")\n",
    "    del _placeholder_samples, _placeholder_prompts, _placeholder_activations\n",
    "\n",
    "    # 4) Multiple replicates per model\n",
    "    all_runs_results_in_domain_list = []\n",
    "    all_runs_ood_summaries_list = []\n",
    "\n",
    "    for i_run, current_seed in enumerate(RANDOM_SEEDS_LIST, start=1):\n",
    "        logger.info(\"=\" * 20 + f\" RUN {i_run}/{N_REPLICATES} (Seed: {current_seed}) \" + \"=\" * 20)\n",
    "\n",
    "        current_device = setup_environment(current_seed, DEVICE_ID)\n",
    "        # Ensure the model is on the current device (usually the same; move if needed)\n",
    "        try:\n",
    "            model_llm.to(current_device)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Model .to({current_device}) raised: {e}\")\n",
    "\n",
    "        # Probes & reports for each run are written in this model's subdirectory\n",
    "        run_identifier = f\"seed_{current_seed}\"\n",
    "        probes_dir_for_run = os.path.join(OUTPUT_DIR, f\"run_{run_identifier}\")\n",
    "        os.makedirs(probes_dir_for_run, exist_ok=True)\n",
    "\n",
    "        # 4.1 Generate training data\n",
    "        all_3way_samples_train = generate_3way_commutativity_data(N_SAMPLES_PER_CLASS_3WAY, seed=current_seed)\n",
    "        prompts_3way_train = [s['prompt'] for s in all_3way_samples_train]\n",
    "\n",
    "        # 4.2 Extract activations\n",
    "        activations_3way_train = get_activations(\n",
    "            model_llm, tokenizer_llm, prompts_3way_train, layers_to_probe, current_device,\n",
    "            batch_size=BATCH_SIZE_PROBE, desc_prefix=f\"TRAIN_COMM_3WAY_Run{i_run}\"\n",
    "        )\n",
    "\n",
    "        # 4.3 Train & evaluate probes (save per layer)\n",
    "        results_3way_this_run = train_and_evaluate_probes(\n",
    "            all_3way_samples_train, activations_3way_train, layers_to_probe, current_device,\n",
    "            log_filename_base=f\"probe_3way_comm_{model_name_short}_{run_identifier}\",\n",
    "            output_dir=probes_dir_for_run,  # write each layer's state_dict and this run's report here\n",
    "            random_seed=current_seed,\n",
    "        )\n",
    "        all_runs_results_in_domain_list.append(results_3way_this_run)\n",
    "\n",
    "        # 4.4 Generate OOD data (2-digit excludes any training prompts from this run)\n",
    "        logger.info(f\"--- Generating OOD Datasets for Run {i_run} ---\")\n",
    "        train_prompt_set_this_run = {s['prompt'] for s in all_3way_samples_train}\n",
    "        ood_datasets_this_run = {\n",
    "            1: generate_ood_comm_samples(1, OOD_TEST_SAMPLES_PER_CLASS[1], seed=current_seed + 1001),\n",
    "            2: generate_ood_comm_samples(2, OOD_TEST_SAMPLES_PER_CLASS[2], seed=current_seed + 1002,\n",
    "                                         exclude_prompts=train_prompt_set_this_run.copy()),\n",
    "            3: generate_ood_comm_samples(3, OOD_TEST_SAMPLES_PER_CLASS[3], seed=current_seed + 1003),\n",
    "            4: generate_ood_comm_samples(4, OOD_TEST_SAMPLES_PER_CLASS[4], seed=current_seed + 1004),\n",
    "        }\n",
    "\n",
    "        # 4.5 OOD evaluation (load each layer's probe; results logged)\n",
    "        ood_summary_this_run = evaluate_all_probes_on_ood(\n",
    "            model_llm, tokenizer_llm, layers_to_probe,\n",
    "            probes_dir_for_run=probes_dir_for_run,  # load layer probes from this run dir\n",
    "            ood_datasets_map=ood_datasets_this_run,\n",
    "            device=current_device,\n",
    "            input_dim_probes=probe_input_dimension_fixed,\n",
    "        )\n",
    "        all_runs_ood_summaries_list.append(ood_summary_this_run)\n",
    "\n",
    "    # 5) Aggregated plots (with CI) — write under this model's output directory\n",
    "    plot_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    generate_final_comparison_plots_with_ci(\n",
    "        all_runs_results_in_domain_list,\n",
    "        all_runs_ood_summaries_list,\n",
    "        layers_to_probe,\n",
    "        OUTPUT_DIR,\n",
    "        plot_timestamp,\n",
    "        model_name_short,\n",
    "        N_REPLICATES\n",
    "    )\n",
    "\n",
    "    logger.info(\"--- All Runs and Aggregated Plotting Finished (this model) ---\")\n",
    "    logger.info(f\"Unified output dir (this model): {OUTPUT_DIR}\")\n",
    "\n",
    "    # 6) Memory cleanup\n",
    "    try:\n",
    "        del model_llm; del tokenizer_llm\n",
    "    except Exception:\n",
    "        pass\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Main (multi-model entry) ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not MODELS:\n",
    "        print(\"[WARN] MODELS is empty. Please add at least one model path or HF repo id to the MODELS list at the top of the script.\")\n",
    "    else:\n",
    "        for midx, model_id in enumerate(MODELS, start=1):\n",
    "            try:\n",
    "                print(f\"\\n===== MODEL {midx}/{len(MODELS)}: {model_id} =====\")\n",
    "                run_experiment_for_model(model_id)\n",
    "            except Exception as e:\n",
    "                # Catch per-model unhandled exceptions and continue to the next model\n",
    "                try:\n",
    "                    logger.exception(f\"[FATAL] Uncaught error while running model {model_id}: {e}\")\n",
    "                except Exception:\n",
    "                    print(f\"[FATAL] Uncaught error while running model {model_id}: {e}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        print(\"\\n>>> All models finished (see per-model logs under ./outputs/<model_name_short>/).\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
